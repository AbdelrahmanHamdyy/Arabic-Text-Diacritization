{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path):\n",
    "\tsentences = []\n",
    "\twith open(path, 'r', encoding='utf-8') as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tsentences.append(line.strip())\n",
    "\n",
    "\treturn sentences\n",
    "\n",
    "PATH = \"../dataset/train.txt\"\n",
    "corpus = readFile(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for sentence in corpus:\n",
    "    sentence = run_buckwalter(sentence)\n",
    "    char_list, _ = extract_labels(sentence)\n",
    "\n",
    "    char_list = [\"\".join(sen) for sen in char_list]\n",
    "    data.append(char_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98820800, 105103400)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining values for parameters\n",
    "embedding_size = 100\n",
    "window_size = 20\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    "\n",
    "fast_Text_model = FastText(\n",
    "                            vector_size=embedding_size,\n",
    "                            window=window_size,\n",
    "                            min_count=min_word,\n",
    "                            sample=down_sampling,\n",
    "                            workers=4,\n",
    "                            epochs=50,\n",
    "                            seed=42,\n",
    "                            sg=1)\n",
    "fast_Text_model.build_vocab(data, progress_per=10000)\n",
    "fast_Text_model.train(data, total_examples=fast_Text_model.corpus_count, epochs=50, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17587607  0.02877168  0.04709973  0.34052846 -0.10787471  0.03396543\n",
      " -0.11186929  0.03204577  0.02601715 -0.04619154 -0.64464664  0.23255761\n",
      " -0.20373139 -0.03959081  0.08147166  0.05366697  0.05857076 -0.03167843\n",
      " -0.06125082  0.17182784 -0.14936416  0.07375959 -0.27469558 -0.34764272\n",
      "  0.29371625 -0.10324     0.08420175 -0.05359089  0.08896402 -0.04694974\n",
      "  0.3940764   0.08949303 -0.5643163  -0.2209296   0.33949855  0.14446706\n",
      "  0.13378167 -0.15082327  0.15100697 -0.05039954  0.28573117  0.02301891\n",
      "  0.01095174 -0.22188954 -0.2218215  -0.28840852 -0.01482417 -0.0590201\n",
      "  0.36846104 -0.03828136 -0.39380917  0.06536474  0.321336    0.25974402\n",
      "  0.27468032 -0.1842511  -0.28447118  0.0251733  -0.09457348  0.19323996\n",
      "  0.25880393 -0.04231093 -0.12811872  0.352846    0.5898262  -0.15524375\n",
      "  0.19650614 -0.0900159  -0.19504753 -0.3754987   0.2750499  -0.06968006\n",
      "  0.26242155  0.08634424  0.2556117  -0.35728014 -0.35281286  0.2248184\n",
      "  0.05599839  0.34907785  0.10232218  0.3731012  -0.0196681   0.08751408\n",
      " -0.47251543 -0.09510664  0.21083805  0.01269392 -0.16203818  0.20815198\n",
      " -0.27576256 -0.17772591 -0.4927883  -0.08658163  0.2343576   0.17162839\n",
      " -0.6776148   0.1278162  -0.01225302 -0.46091643]\n",
      "0.56691873\n"
     ]
    }
   ],
   "source": [
    "fast_Text_model.save(\"./models/ft_model\")\n",
    "print(fast_Text_model.wv[buckwalter.transliterate(\"ياكل\")])\n",
    "print(fast_Text_model.wv.similarity(buckwalter.transliterate(\"احمد\"), buckwalter.transliterate(\"محمد\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "word_count = 0\n",
    "for sentence in corpus:\n",
    "    sentence = run_buckwalter(sentence)\n",
    "    char_list, _ = extract_labels(sentence)\n",
    "\n",
    "    char_list = [\"\".join(sen) for sen in char_list]\n",
    "    joined_with_space = \" \".join(char_list)\n",
    "    word_count += len(joined_with_space)\n",
    "    data.append(joined_with_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10403546\n"
     ]
    }
   ],
   "source": [
    "print(len(data), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_idf_model = TfidfVectorizer(lowercase=False)\n",
    "tf_idf_vector = tr_idf_model.fit_transform(data)\n",
    "words_set = tr_idf_model.get_feature_names_out()\n",
    "df_tf_idf = pd.DataFrame(columns=words_set)\n",
    "chunk_size=1000\n",
    "for i in range(0, tf_idf_vector.shape[0], chunk_size):\n",
    "        end_idx = min(i + chunk_size, tf_idf_vector.shape[0])\n",
    "        tf_idf_chunk = tf_idf_vector[i:end_idx].toarray()\n",
    "        chunk_df = pd.DataFrame(tf_idf_chunk, columns=words_set)\n",
    "        df_tf_idf = pd.concat([df_tf_idf, chunk_df], ignore_index=True)\n",
    "df_tf_idf.to_csv('models/tf_idf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DataFrame from the CSV file\n",
    "df_from_csv = pd.read_csv('your_dataframe.csv')\n",
    "\n",
    "# Accessing TF-IDF values for the word 'example'\n",
    "tf_idf_for_example = df_from_csv['example']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "X_array = X.toarray()\n",
    "\n",
    "df = pd.DataFrame(data=X_array, columns=feature_names, index=data)\n",
    "\n",
    "df.to_csv('models/bag_of_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocesses text input in a way that BERT can interpret.\n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.encode(marked_text, max_length=512, truncation=True, padding=True)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    # convert inputs to tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    return tokenized_text, tokens_tensor, segments_tensor\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "        # obtain hidden states\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "        # \"token\" is a [12 x 768] tensor\n",
    "        # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    return token_vecs_sum\n",
    "\n",
    "def visualize_embeddings(context_tokens, context_embeddings):\n",
    "    filepath = \"models/embeddings.tsv\"\n",
    "    with open(filepath, 'w+') as file_metadata:\n",
    "        for i, token in enumerate(context_tokens):\n",
    "            file_metadata.write(token + '\\n')\n",
    "    with open(filepath, 'w+') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        for embedding in context_embeddings:\n",
    "            writer.writerow(embedding.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "for sentence in data:\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(\n",
    "        sentence, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(\n",
    "        tokens_tensor, segments_tensors, model)\n",
    "    # make ordered dictionary to keep track of the position of each   word\n",
    "    tokens = OrderedDict()\n",
    "    # loop over tokens in sensitive sentence\n",
    "    for token in tokenized_text[1:-1]:\n",
    "        # keep track of position of word and whether it occurs multiple times\n",
    "        if token in tokens:\n",
    "            tokens[token] += 1\n",
    "        else:\n",
    "            tokens[token] = 1\n",
    "        # compute the position of the current token\n",
    "        token_indices = [i for i, t in enumerate(\n",
    "            tokenized_text) if t == token]\n",
    "        current_index = token_indices[tokens[token]-1]\n",
    "        # get the corresponding embedding\n",
    "        token_vec = list_token_embeddings[current_index]\n",
    "        # save values\n",
    "        context_tokens.append(token)\n",
    "        context_embeddings.append(token_vec)\n",
    "\n",
    "visualize_embeddings(context_tokens, context_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56691873\n"
     ]
    }
   ],
   "source": [
    "# word_embeddings_fasttext()\n",
    "loaded_model = FastText.load(\"./models/ft_model\")\n",
    "print(loaded_model.wv.similarity(buckwalter.transliterate(\"احمد\"), buckwalter.transliterate(\"محمد\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

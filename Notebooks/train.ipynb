{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Char [['m', 's', '>', 'l', 'p'], ['w', 'm', 'n'], ['H', 'n', 'v'], ['w', 'h', 'w'], ['q', 'A', 'd', 'r'], ['E', 'l', 'Y'], ['A', 'l', '<', 'T', 'E', 'A', 'm'], ['>', 'w'], ['A', 'l', 'k', 's', 'w', 'p'], ['>', 'w'], ['A', 'l', 'E', 't', 'q'], ['v', 'm'], ['A', 'f', 't', 'q', 'r'], ['f', 'E', 'j', 'z'], ['E', 'n'], ['k', 'l'], ['*', 'l', 'k'], ['l', 'm'], ['y', 'j', 'z', 'h'], ['A', 'l', 'S', 'w', 'm'], ['>', 'S', 'l', 'A']]\n",
      "Diac [['a', 'o', 'a', 'a', 'N'], ['a', 'a', 'o'], ['a', 'i', 'a'], ['a', 'u', 'a'], ['a', ' ', 'i', 'N'], ['a', 'a', ' '], [' ', ' ', 'i', 'o', 'a', ' ', 'i'], ['a', 'o'], [' ', 'o', 'i', 'o', 'a', 'i'], ['a', 'o'], [' ', 'o', 'i', 'o', 'i'], ['u', '~a'], [' ', 'o', 'a', 'a', 'a'], ['a', 'a', 'a', 'a'], ['a', 'o'], ['u', '~i'], ['a', 'i', 'a'], ['a', 'o'], ['u', 'o', 'i', 'i'], [' ', ' ', '~a', 'o', 'u'], ['a', 'o', 'F', ' ']]\n",
      "38\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim = 200, hidden_size = 256,num_layers=3):\n",
    "        \"\"\"\n",
    "        The constructor of our RNN model\n",
    "        Inputs:\n",
    "        - vacab_size: the number of unique characters\n",
    "        - embedding_dim: the embedding dimension\n",
    "        - n_classes: the number of final classes (diacritics)\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # (1) Create the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size,num_layers=num_layers, batch_first=True,bidirectional=True)\n",
    "\n",
    "        # (3) Create a linear layer with number of neorons = n_classes\n",
    "        self.linear = nn.Linear(hidden_size*2, n_classes)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        \"\"\"\n",
    "        This function does the forward pass of our model\n",
    "        Inputs:\n",
    "        - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "        Returns:\n",
    "        - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        final_output = None\n",
    "        \n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        final_output = F.softmax(output, dim=1)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"./models/lstm.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, train_labels, batch_size=512, epochs=20, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    This function implements the training logic\n",
    "    Inputs:\n",
    "    - model: the model to be trained\n",
    "    - train_dataset: the training set\n",
    "    - batch_size: integer represents the number of examples per step\n",
    "    - epochs: integer represents the total number of epochs (full training pass)\n",
    "    - learning_rate: the learning rate to be used by the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    tensor_train_dataset = TensorDataset(train_dataset, train_labels)\n",
    "    train_dataloader = DataLoader(tensor_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # (3) create the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # GPU configuration\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            # (4) move the train input to the device\n",
    "            train_label = train_label.to(device)\n",
    "\n",
    "            # (5) move the train label to the device\n",
    "            train_input = train_input.to(device)\n",
    "\n",
    "            # (6) do the forward pass\n",
    "            output = model(train_input).float()\n",
    "\n",
    "            # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "            one_hot_train_label = F.one_hot(train_label, num_classes=15).float()\n",
    "            batch_loss = criterion(output, one_hot_train_label)\n",
    "\n",
    "            # (8) append the batch loss to the total_loss_train\n",
    "            total_loss_train += batch_loss\n",
    "            \n",
    "            # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "            # Compare predicted diacritic with true diacritic and count correct predictions\n",
    "            correct_predictions = (output.argmax(dim=2) == train_label)\n",
    "\n",
    "            # Calculate accuracy for the current batch\n",
    "            acc = correct_predictions.sum().item()\n",
    "\n",
    "            total_acc_train += acc\n",
    "\n",
    "            # (10) zero your gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # (11) do the backward pass\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # (12) update the weights with your optimizer\n",
    "            optimizer.step()     \n",
    "        \n",
    "        # epoch loss\n",
    "        epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "        # (13) calculate the accuracy\n",
    "        epoch_acc = total_acc_train / (len(train_dataset) * len(train_dataset[0]))\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "            | Train Accuracy: {epoch_acc}\\n')\n",
    "        if epoch_acc > best_accuracy:\n",
    "            best_accuracy = epoch_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'Saved the best model with accuracy: {best_accuracy} to {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=  readFile(TRAIN_PATH)\n",
    "valid_corpus = readFile(VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "max_sequence_length = 0\n",
    "\n",
    "for sentence in corpus[:500]:\n",
    "\t# Clean each sentence in the corpus\n",
    "\tclean_sentence = run_buckwalter(sentence.strip())\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\tchar_list, diacritics_list = extract_labels(clean_sentence)\n",
    "\n",
    "\tX_train.append(char_list)\n",
    "\ty_train.append(diacritics_list)\n",
    "\n",
    "\t# Get the max sequence length and concatenate the embeddings of the words\n",
    "\tfor word in char_list:\n",
    "\t\tmax_sequence_length = max(max_sequence_length, len(word))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([2, 1, 5, 5]), tensor([2, 1]), tensor([2, 2, 2]), tensor([0, 1, 2, 9, 5]), tensor([2, 2, 5]), tensor([0, 2, 1]), tensor([2, 0, 2]), tensor([ 0,  0,  9,  1,  2,  3, 11]), tensor([0, 1, 5]), tensor([2, 2, 2, 2]), tensor([2, 1, 5, 5]), tensor([3, 2, 1, 8]), tensor([2, 1, 2, 3, 0, 0]), tensor([2, 3, 1, 2, 0, 3]), tensor([2, 1, 3]), tensor([2, 3, 0, 8]), tensor([3, 0, 1, 3, 1, 2, 0, 3]), tensor([5, 5, 0, 2]), tensor([2, 0]), tensor([5, 3, 2]), tensor([5, 5, 0, 5, 5]), tensor([3, 1]), tensor([ 0,  0, 10,  0,  3]), tensor([2, 5, 0, 2, 7]), tensor([2, 3, 1, 2, 0, 3]), tensor([5, 1, 2, 8]), tensor([3, 2, 2, 8]), tensor([ 2,  2, 10]), tensor([5, 9, 0, 8]), tensor([0, 1, 5]), tensor([2, 2, 2, 2]), tensor([2, 1, 5]), tensor([0, 1, 3]), tensor([2, 0, 8]), tensor([2, 1]), tensor([3, 3, 1, 8]), tensor([2, 2, 2, 9, 5, 5]), tensor([5, 2]), tensor([2, 5, 1, 3]), tensor([ 0,  0, 11,  9,  0,  3]), tensor([2, 3, 1, 2, 0, 3]), tensor([0, 1, 5, 1, 2, 3]), tensor([3, 0]), tensor([2, 3, 0, 3]), tensor([0, 0, 9, 2, 0, 2, 3]), tensor([ 2,  0,  0, 11,  5,  0,  3]), tensor([3, 0, 9, 2, 3]), tensor([2, 2, 1, 3]), tensor([2, 3, 2]), tensor([2, 3, 1, 8]), tensor([5, 2, 9, 6]), tensor([2, 1, 5]), tensor([2, 0, 3, 8]), tensor([2, 2, 1, 2, 0, 3, 3]), tensor([2, 9]), tensor([0, 0, 9, 0, 3, 2]), tensor([2, 0, 3, 6]), tensor([3, 2, 0, 9, 3]), tensor([2, 2, 0, 2, 0]), tensor([2, 0, 2]), tensor([2, 0, 3, 6]), tensor([5, 2]), tensor([ 2,  0,  0, 10,  1,  3,  0,  3]), tensor([0, 2, 0]), tensor([2, 3, 2]), tensor([ 0,  0, 10,  1,  2]), tensor([3, 2, 1, 3, 3]), tensor([5, 3, 2]), tensor([2, 2, 1]), tensor([5, 1, 2, 2, 1]), tensor([2, 1, 5, 5]), tensor([3, 2, 2, 3]), tensor([2, 0]), tensor([2, 2, 2, 9, 5]), tensor([0, 2, 1]), tensor([2, 1]), tensor([0, 1, 2, 3, 9, 5]), tensor([2, 1, 5, 5]), tensor([2, 0]), tensor([2, 9]), tensor([2, 1]), tensor([5, 2, 1, 2]), tensor([2, 1, 3]), tensor([0, 1, 2, 1, 3]), tensor([2, 2, 1]), tensor([2, 2, 1]), tensor([0, 1, 2, 2, 2]), tensor([2, 2, 0]), tensor([2, 1, 2, 1, 0]), tensor([2, 5]), tensor([3, 2, 0, 8]), tensor([2, 1]), tensor([2, 1, 5, 0, 5]), tensor([2, 0, 7]), tensor([2, 2, 0]), tensor([2, 2, 2]), tensor([2, 5]), tensor([3, 1, 2]), tensor([0, 1, 2, 1, 3]), tensor([2, 1]), tensor([2, 1, 5, 5]), tensor([0, 1, 2, 3, 9, 5]), tensor([2, 1]), tensor([5, 1, 2, 2, 0]), tensor([2, 5]), tensor([2, 0, 6]), tensor([2, 5, 1, 2, 5]), tensor([3, 1]), tensor([2, 1, 3, 3]), tensor([0, 1, 0, 3, 0]), tensor([2, 2, 0]), tensor([2, 1]), tensor([2, 1]), tensor([2, 5, 1]), tensor([3, 1]), tensor([2, 0, 3, 0]), tensor([2, 2, 0]), tensor([3, 1]), tensor([2, 2, 3, 0]), tensor([2, 9, 2, 0]), tensor([2, 0]), tensor([2, 1, 5, 5]), tensor([2, 3, 2, 0, 2, 5]), tensor([0, 1, 2, 1, 3]), tensor([2, 2, 1]), tensor([2, 1]), tensor([2, 5, 1]), tensor([3, 1]), tensor([2, 0, 3, 0]), tensor([2, 2, 0]), tensor([3, 1]), tensor([2, 2, 3, 0]), tensor([2, 1]), tensor([2, 2, 2, 9, 1]), tensor([2, 2, 5, 5]), tensor([0, 1]), tensor([2, 0, 2, 1]), tensor([0, 1, 2, 2, 1]), tensor([0]), tensor([0]), tensor([0, 0]), tensor([2, 1, 5, 5]), tensor([2, 5, 1, 2, 0]), tensor([2, 0, 3, 2, 7]), tensor([3, 1, 2, 0]), tensor([0, 2, 1]), tensor([2, 2, 0]), tensor([2, 1]), tensor([2, 0, 2, 1]), tensor([2, 1, 5, 0, 2, 7]), tensor([3, 1, 2]), tensor([0, 1, 2, 3, 9, 3]), tensor([2, 0, 1, 2, 1, 3]), tensor([2, 2, 0]), tensor([2, 5, 0, 5]), tensor([2, 1]), tensor([5, 1, 2, 0]), tensor([2, 0, 3, 2, 7]), tensor([3, 1]), tensor([2, 1, 3]), tensor([2, 2, 3, 3]), tensor([3, 0]), tensor([ 0,  0, 11,  0,  2,  2,  1,  3]), tensor([2, 3, 1]), tensor([2, 2, 0, 2, 2, 0]), tensor([3, 2, 9, 5]), tensor([5, 1, 6]), tensor([2, 2, 0]), tensor([2, 1, 5, 0, 8]), tensor([5, 1, 3, 0]), tensor([2, 3, 2, 0, 2, 6]), tensor([2, 0, 2]), tensor([0]), tensor([0]), tensor([2, 1, 5, 5]), tensor([2, 0, 3, 2, 7]), tensor([3, 1, 2, 0]), tensor([2, 1]), tensor([2, 0, 3, 2, 7]), tensor([2, 2, 0]), tensor([2, 5, 0, 5]), tensor([2, 1]), tensor([5, 1, 2, 0]), tensor([3, 1, 2, 1, 3]), tensor([3, 1]), tensor([2, 0, 2, 1, 3]), tensor([3, 2, 9, 5]), tensor([2, 0]), tensor([5, 2, 9, 0]), tensor([2, 0, 7]), tensor([2, 2, 1, 5, 5]), tensor([2, 2, 0]), tensor([2, 5, 0, 5]), tensor([2, 1]), tensor([5, 1, 2, 0]), tensor([2, 0, 3, 2, 7]), tensor([3, 1]), tensor([2, 1, 3]), tensor([2, 2, 3, 3]), tensor([2, 2, 1, 2, 3, 0]), tensor([2, 1]), tensor([5, 2, 0, 2]), tensor([3, 1, 5]), tensor([2, 3, 2]), tensor([3, 0]), tensor([0, 1, 2, 3, 9, 0, 3]), tensor([0]), tensor([0]), tensor([2, 2, 2, 2, 0, 6]), tensor([2, 1, 5]), tensor([2, 1, 5, 0, 8]), tensor([2, 0, 3, 2, 6]), tensor([2, 0, 2]), tensor([2, 1, 5, 5, 1]), tensor([5, 1, 2, 5]), tensor([3, 1]), tensor([2, 1, 3]), tensor([2, 2, 0, 3]), tensor([0, 1, 3, 1, 3]), tensor([2, 2, 5]), tensor([2, 2, 0, 3]), tensor([ 5,  3, 10]), tensor([0, 1, 2, 1, 2, 3]), tensor([2, 0, 1, 2, 2, 0, 3, 3]), tensor([3, 1]), tensor([2, 2, 0, 3, 0, 2]), tensor([2, 2, 2, 0, 3, 2]), tensor([2, 2, 2, 0, 3, 3]), tensor([2, 1, 2, 0, 8]), tensor([2, 2, 1, 5, 5]), tensor([2, 3, 2, 1, 5]), tensor([3, 2, 0]), tensor([5, 1, 2, 0]), tensor([5, 3, 0, 5]), tensor([2, 1, 2, 5]), tensor([3, 2, 9]), tensor([0, 1, 2, 1, 2]), tensor([2, 1, 2, 5, 2, 0]), tensor([2, 1, 5, 0, 6]), tensor([3, 0]), tensor([ 2, 10]), tensor([ 0,  1,  0,  2,  0,  3, 10]), tensor([2, 1, 5, 5]), tensor([2, 2, 1]), tensor([0, 1]), tensor([2, 9, 2, 2, 0]), tensor([2, 1, 2]), tensor([2, 1, 3, 3]), tensor([2, 3, 1, 5, 5]), tensor([2, 2]), tensor([0, 1, 2, 1, 3]), tensor([2, 3, 1]), tensor([2, 0, 2]), tensor([ 0,  1,  2,  0,  2,  1,  3, 11]), tensor([ 2,  0,  0, 11,  0,  2,  0,  3, 11]), tensor([2, 1, 3, 0, 7, 0]), tensor([2, 2, 0]), tensor([2, 3, 2]), tensor([ 5,  2, 10,  5,  5]), tensor([0, 1, 2, 1, 2, 2]), tensor([3, 1]), tensor([3, 2, 0, 2, 3]), tensor([0, 1, 2, 2, 3]), tensor([2, 2, 0]), tensor([2, 1]), tensor([2, 1, 2, 2, 2, 0]), tensor([2, 2, 0, 3, 2]), tensor([2, 3, 9, 5]), tensor([2, 2, 2, 9, 5]), tensor([2, 0, 3, 5]), tensor([2, 2, 0, 3, 3]), tensor([0, 1, 2, 2, 3]), tensor([2, 1]), tensor([2, 3, 1]), tensor([2, 1]), tensor([2, 2, 0]), tensor([2, 5, 0, 5]), tensor([2, 1]), tensor([2, 5, 0, 2]), tensor([2, 1]), tensor([2, 1, 3]), tensor([0, 1, 2, 2, 0, 3]), tensor([2, 5, 1, 3, 2]), tensor([2, 1]), tensor([2, 1, 8]), tensor([2, 9]), tensor([2, 2, 1, 3]), tensor([3, 2, 9, 2, 0]), tensor([2, 9, 0, 2, 6]), tensor([2, 0, 3, 2, 6]), tensor([2, 2, 1]), tensor([2, 5, 1]), tensor([3, 0, 2, 0]), tensor([2, 2, 0, 3, 3]), tensor([0, 1, 2, 9, 0, 2, 0, 3]), tensor([2, 1]), tensor([2, 2, 2]), tensor([2, 2, 0, 2, 6]), tensor([2, 1, 7, 0]), tensor([ 3,  0,  0, 10,  1,  3]), tensor([3, 2, 1, 2, 5, 0, 2, 0]), tensor([3, 0, 0, 9, 3, 2, 3]), tensor([2, 2, 0, 2]), tensor([2, 2, 5, 5, 1]), tensor([2, 2, 2, 2]), tensor([0, 1, 2, 0, 3]), tensor([2, 1, 2]), tensor([0, 1, 2, 1, 3]), tensor([ 5, 13,  0]), tensor([2, 2, 2, 2]), tensor([0, 0, 9, 0, 3, 5]), tensor([2, 2, 2, 2]), tensor([2, 1, 2]), tensor([0, 1, 2, 1, 3]), tensor([2, 3, 0, 7, 0]), tensor([2, 2, 1]), tensor([2, 0, 2]), tensor([2, 3, 2]), tensor([3, 3, 1, 3]), tensor([ 5, 10]), tensor([2, 0, 3, 8]), tensor([3, 1, 5, 1]), tensor([ 2,  0,  1,  5, 11]), tensor([2, 1, 2, 5, 1]), tensor([2, 2, 1, 3, 5]), tensor([ 2, 11]), tensor([ 0,  1,  5, 10]), tensor([3, 5, 5, 3]), tensor([ 0,  1,  5, 10]), tensor([2, 2, 0]), tensor([0, 0, 9, 0, 3, 3]), tensor([2, 0, 0, 9, 3, 0, 5]), tensor([2, 1, 2, 5, 1]), tensor([2, 1, 7, 0]), tensor([2, 1]), tensor([2, 1, 3, 5]), tensor([5, 2]), tensor([2, 1, 7, 0]), tensor([2, 2, 1, 3, 2, 0]), tensor([3, 5, 5, 2, 1]), tensor([0, 0, 9, 3, 0, 3]), tensor([0, 9, 3, 0]), tensor([2, 2, 2]), tensor([2, 1, 2]), tensor([2, 1, 3]), tensor([2, 3, 0, 3]), tensor([ 2, 10]), tensor([0, 1, 2, 1, 3]), tensor([2, 2, 1]), tensor([2, 2, 5, 0, 0]), tensor([3, 2, 0]), tensor([0, 1, 8]), tensor([ 2,  0,  1,  5, 11]), tensor([5, 5, 5, 5]), tensor([ 3,  2, 10]), tensor([0, 1, 2, 1, 3]), tensor([2, 5, 5, 2, 0, 5]), tensor([2, 5, 2, 0]), tensor([2, 2, 1, 2, 2, 0, 3]), tensor([5, 1, 2, 0, 2]), tensor([5, 5, 3]), tensor([0, 1, 2, 1, 3]), tensor([2, 2, 9, 0]), tensor([ 2, 11]), tensor([0, 0, 9, 3, 0, 3]), tensor([2, 2, 1, 2, 5]), tensor([2, 1, 2, 0, 3, 3]), tensor([2, 5]), tensor([ 2,  3,  2, 10]), tensor([0, 1, 2, 1, 3]), tensor([5, 5, 5, 5]), tensor([3, 2, 9]), tensor([5, 5, 2, 1]), tensor([0, 0, 9, 3, 0, 3]), tensor([5, 3, 2]), tensor([2, 1, 7, 0]), tensor([2, 5, 2]), tensor([2, 5]), tensor([2, 5, 5, 5, 5]), tensor([5, 3, 2]), tensor([ 3,  2, 14]), tensor([2, 2, 2, 1, 3]), tensor([5, 1, 2, 0, 5]), tensor([0, 1, 2, 1, 3]), tensor([3, 0]), tensor([2, 1, 3]), tensor([5, 5, 2, 1]), tensor([2, 3, 2]), tensor([2, 2, 0]), tensor([3, 0]), tensor([0, 1, 5, 5, 0, 2, 1, 3]), tensor([3, 1]), tensor([2, 1, 2, 0, 3]), tensor([0, 1, 5, 2, 0, 2, 2, 3])]\n",
      "tensor([[2, 1, 5,  ..., 0, 0, 0],\n",
      "        [2, 1, 0,  ..., 0, 0, 0],\n",
      "        [2, 2, 2,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [3, 1, 0,  ..., 0, 0, 0],\n",
      "        [2, 1, 2,  ..., 0, 0, 0],\n",
      "        [0, 1, 5,  ..., 2, 3, 0]])\n"
     ]
    }
   ],
   "source": [
    "X_train_padded = [torch.tensor([char_to_index[char] for char in word]) for sentence in X_train for word in sentence ]\n",
    "X_train_padded = pad_sequence(X_train_padded, batch_first=True)\n",
    "\n",
    "y_train_padded = [torch.tensor([diacritic_to_index[char] for char in word]) for sentence in y_train for word in sentence ]\n",
    "print(y_train_padded)\n",
    "y_train_padded = pad_sequence(y_train_padded, batch_first=True)\n",
    "print(y_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(39, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=3, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=RNN(len(unique_characters) + 1, len(unique_diacritics), embedding_dim=200, hidden_size=256, num_layers=3)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.0031970313284546137             | Train Accuracy: 0.5645798082346306\n",
      "\n",
      "Saved the best model with accuracy: 0.5645798082346306 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.0033756198827177286             | Train Accuracy: 0.07416807670614778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.0033158729784190655             | Train Accuracy: 0.36661026508742245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 0.0032705615740269423             | Train Accuracy: 0.40073322053017485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss: 0.003230523318052292             | Train Accuracy: 0.40552735476593343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss: 0.0031993058510124683             | Train Accuracy: 0.5470953186689227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss: 0.003184766974300146             | Train Accuracy: 0.6130851663846588\n",
      "\n",
      "Saved the best model with accuracy: 0.6130851663846588 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss: 0.003173203906044364             | Train Accuracy: 0.6012408347433729\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss: 0.0031626231502741575             | Train Accuracy: 0.6144952058657642\n",
      "\n",
      "Saved the best model with accuracy: 0.6144952058657642 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss: 0.0031526340171694756             | Train Accuracy: 0.6353637901861252\n",
      "\n",
      "Saved the best model with accuracy: 0.6353637901861252 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss: 0.0031453189440071583             | Train Accuracy: 0.6472081218274112\n",
      "\n",
      "Saved the best model with accuracy: 0.6472081218274112 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss: 0.003138156607747078             | Train Accuracy: 0.6635645798082346\n",
      "\n",
      "Saved the best model with accuracy: 0.6635645798082346 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss: 0.0031297567766159773             | Train Accuracy: 0.6703327693175409\n",
      "\n",
      "Saved the best model with accuracy: 0.6703327693175409 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss: 0.003120654495432973             | Train Accuracy: 0.670614777213762\n",
      "\n",
      "Saved the best model with accuracy: 0.670614777213762 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss: 0.003112019505351782             | Train Accuracy: 0.682741116751269\n",
      "\n",
      "Saved the best model with accuracy: 0.682741116751269 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | Train Loss: 0.0031042310874909163             | Train Accuracy: 0.6841511562323745\n",
      "\n",
      "Saved the best model with accuracy: 0.6841511562323745 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | Train Loss: 0.003097464796155691             | Train Accuracy: 0.6830231246474902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 18 | Train Loss: 0.0030910351779311895             | Train Accuracy: 0.6751269035532995\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 19 | Train Loss: 0.003086255630478263             | Train Accuracy: 0.6835871404399323\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 20 | Train Loss: 0.003081738715991378             | Train Accuracy: 0.6897913141567964\n",
      "\n",
      "Saved the best model with accuracy: 0.6897913141567964 to ./models/lstm.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, X_train_padded, y_train_padded, batch_size=512, epochs=20, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

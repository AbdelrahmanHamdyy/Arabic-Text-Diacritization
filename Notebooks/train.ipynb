{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Char [['m', 's', '>', 'l', 'p'], ['w', 'm', 'n'], ['H', 'n', 'v'], ['w', 'h', 'w'], ['q', 'A', 'd', 'r'], ['E', 'l', 'Y'], ['A', 'l', '<', 'T', 'E', 'A', 'm'], ['>', 'w'], ['A', 'l', 'k', 's', 'w', 'p'], ['>', 'w'], ['A', 'l', 'E', 't', 'q'], ['v', 'm'], ['A', 'f', 't', 'q', 'r'], ['f', 'E', 'j', 'z'], ['E', 'n'], ['k', 'l'], ['*', 'l', 'k'], ['l', 'm'], ['y', 'j', 'z', 'h'], ['A', 'l', 'S', 'w', 'm'], ['>', 'S', 'l', 'A']]\n",
      "Diac [['a', 'o', 'a', 'a', 'N'], ['a', 'a', 'o'], ['a', 'i', 'a'], ['a', 'u', 'a'], ['a', ' ', 'i', 'N'], ['a', 'a', ' '], [' ', ' ', 'i', 'o', 'a', ' ', 'i'], ['a', 'o'], [' ', 'o', 'i', 'o', 'a', 'i'], ['a', 'o'], [' ', 'o', 'i', 'o', 'i'], ['u', '~a'], [' ', 'o', 'a', 'a', 'a'], ['a', 'a', 'a', 'a'], ['a', 'o'], ['u', '~i'], ['a', 'i', 'a'], ['a', 'o'], ['u', 'o', 'i', 'i'], [' ', ' ', '~a', 'o', 'u'], ['a', 'o', 'F', ' ']]\n",
      "38\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim = 200, hidden_size = 256,num_layers=3):\n",
    "        \"\"\"\n",
    "        The constructor of our RNN model\n",
    "        Inputs:\n",
    "        - vacab_size: the number of unique characters\n",
    "        - embedding_dim: the embedding dimension\n",
    "        - n_classes: the number of final classes (diacritics)\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # (1) Create the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size,num_layers=num_layers, batch_first=True,bidirectional=True)\n",
    "\n",
    "        # (3) Create a linear layer with number of neorons = n_classes\n",
    "        self.linear = nn.Linear(hidden_size*2, n_classes)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        \"\"\"\n",
    "        This function does the forward pass of our model\n",
    "        Inputs:\n",
    "        - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "        Returns:\n",
    "        - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        final_output = None\n",
    "        \n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        # final_output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"./models/lstm.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, train_labels, batch_size=512, epochs=5, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    This function implements the training logic\n",
    "    Inputs:\n",
    "    - model: the model to be trained\n",
    "    - train_dataset: the training set\n",
    "    - batch_size: integer represents the number of examples per step\n",
    "    - epochs: integer represents the total number of epochs (full training pass)\n",
    "    - learning_rate: the learning rate to be used by the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    tensor_train_dataset = TensorDataset(train_dataset, train_labels)\n",
    "    train_dataloader = DataLoader(tensor_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # (3) create the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # GPU configuration\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            # (4) move the train input to the device\n",
    "            train_label = train_label.to(device)\n",
    "\n",
    "            # (5) move the train label to the device\n",
    "            train_input = train_input.to(device)\n",
    "\n",
    "            # (6) do the forward pass\n",
    "            output = model(train_input)\n",
    "\n",
    "            # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "\n",
    "            # (8) append the batch loss to the total_loss_train\n",
    "            total_loss_train += batch_loss\n",
    "            \n",
    "            # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "            acc = (output.argmax(2) == train_label).sum().item()\n",
    "\n",
    "            total_acc_train += acc\n",
    "\n",
    "            # (10) zero your gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # (11) do the backward pass\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # (12) update the weights with your optimizer\n",
    "            optimizer.step()     \n",
    "        \n",
    "        # epoch loss\n",
    "        epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "        # (13) calculate the accuracy\n",
    "        epoch_acc = total_acc_train / (len(train_dataset) * train_dataset[0][0].shape[0])\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "            | Train Accuracy: {epoch_acc}\\n')\n",
    "        if epoch_acc > best_accuracy:\n",
    "            best_accuracy = epoch_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'Saved the best model with accuracy: {best_accuracy} to {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=  readFile(TRAIN_PATH)\n",
    "valid_corpus = readFile(VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "max_sequence_length = 0\n",
    "\n",
    "for sentence in corpus[:10]:\n",
    "\t# Clean each sentence in the corpus\n",
    "\tclean_sentence = run_buckwalter(sentence.strip())\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\tchar_list, diacritics_list = extract_labels(clean_sentence)\n",
    "\n",
    "\tX_train.append(char_list)\n",
    "\ty_train.append(diacritics_list)\n",
    "\n",
    "\t# Get the max sequence length and concatenate the embeddings of the words\n",
    "\tfor word in char_list:\n",
    "\t\tmax_sequence_length = max(max_sequence_length, len(word))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([2, 1, 5, 5]), tensor([2, 1]), tensor([2, 2, 2]), tensor([ 9,  1,  2, 10,  5]), tensor([2, 2, 5]), tensor([9, 2, 1]), tensor([2, 9, 2]), tensor([ 9,  9, 10,  1,  2,  3, 12]), tensor([9, 1, 5]), tensor([2, 2, 2, 2]), tensor([2, 1, 5, 5]), tensor([3, 2, 1, 8]), tensor([2, 1, 2, 3, 9, 9]), tensor([2, 3, 1, 2, 9, 3]), tensor([2, 1, 3]), tensor([2, 3, 9, 8]), tensor([3, 9, 1, 3, 1, 2, 9, 3]), tensor([5, 5, 9, 2]), tensor([2, 9]), tensor([5, 3, 2]), tensor([5, 5, 9, 5, 5]), tensor([3, 1]), tensor([ 9,  9, 11,  9,  3]), tensor([2, 5, 9, 2, 7]), tensor([2, 3, 1, 2, 9, 3]), tensor([5, 1, 2, 8]), tensor([3, 2, 2, 8]), tensor([ 2,  2, 11]), tensor([ 5, 10,  9,  8]), tensor([9, 1, 5]), tensor([2, 2, 2, 2]), tensor([2, 1, 5]), tensor([9, 1, 3]), tensor([2, 9, 8]), tensor([2, 1]), tensor([3, 3, 1, 8]), tensor([ 2,  2,  2, 10,  5,  5]), tensor([5, 2]), tensor([2, 5, 1, 3]), tensor([ 9,  9, 12, 10,  9,  3]), tensor([2, 3, 1, 2, 9, 3]), tensor([9, 1, 5, 1, 2, 3]), tensor([3, 9]), tensor([2, 3, 9, 3]), tensor([ 9,  9, 10,  2,  9,  2,  3]), tensor([ 2,  9,  9, 12,  5,  9,  3]), tensor([ 3,  9, 10,  2,  3]), tensor([2, 2, 1, 3]), tensor([2, 3, 2]), tensor([2, 3, 1, 8]), tensor([ 5,  2, 10,  6]), tensor([2, 1, 5]), tensor([2, 9, 3, 8]), tensor([2, 2, 1, 2, 9, 3, 3]), tensor([ 2, 10]), tensor([ 9,  9, 10,  9,  3,  2]), tensor([2, 9, 3, 6]), tensor([ 3,  2,  9, 10,  3]), tensor([2, 2, 9, 2, 9]), tensor([2, 9, 2]), tensor([2, 9, 3, 6]), tensor([5, 2]), tensor([ 2,  9,  9, 11,  1,  3,  9,  3]), tensor([9, 2, 9]), tensor([2, 3, 2]), tensor([ 9,  9, 11,  1,  2]), tensor([3, 2, 1, 3, 3]), tensor([5, 3, 2]), tensor([2, 2, 1]), tensor([5, 1, 2, 2, 1]), tensor([2, 1, 5, 5]), tensor([3, 2, 2, 3]), tensor([2, 9]), tensor([ 2,  2,  2, 10,  5]), tensor([9, 2, 1]), tensor([2, 1]), tensor([ 9,  1,  2,  3, 10,  5]), tensor([2, 1, 5, 5]), tensor([2, 9]), tensor([ 2, 10]), tensor([2, 1]), tensor([5, 2, 1, 2]), tensor([2, 1, 3]), tensor([9, 1, 2, 1, 3]), tensor([2, 2, 1]), tensor([2, 2, 1]), tensor([9, 1, 2, 2, 2]), tensor([2, 2, 9]), tensor([2, 1, 2, 1, 9]), tensor([2, 5]), tensor([3, 2, 9, 8]), tensor([2, 1]), tensor([2, 1, 5, 9, 5]), tensor([2, 9, 7]), tensor([2, 2, 9]), tensor([2, 2, 2]), tensor([2, 5]), tensor([3, 1, 2]), tensor([9, 1, 2, 1, 3]), tensor([2, 1]), tensor([2, 1, 5, 5]), tensor([ 9,  1,  2,  3, 10,  5]), tensor([2, 1]), tensor([5, 1, 2, 2, 9]), tensor([2, 5]), tensor([2, 9, 6]), tensor([2, 5, 1, 2, 5]), tensor([3, 1]), tensor([2, 1, 3, 3]), tensor([9, 1, 9, 3, 9]), tensor([2, 2, 9]), tensor([2, 1]), tensor([2, 1]), tensor([2, 5, 1]), tensor([3, 1]), tensor([2, 9, 3, 9]), tensor([2, 2, 9]), tensor([3, 1]), tensor([2, 2, 3, 9]), tensor([ 2, 10,  2,  9]), tensor([2, 9]), tensor([2, 1, 5, 5]), tensor([2, 3, 2, 9, 2, 5]), tensor([9, 1, 2, 1, 3]), tensor([2, 2, 1]), tensor([2, 1]), tensor([2, 5, 1]), tensor([3, 1]), tensor([2, 9, 3, 9]), tensor([2, 2, 9]), tensor([3, 1]), tensor([2, 2, 3, 9]), tensor([2, 1]), tensor([ 2,  2,  2, 10,  1]), tensor([2, 2, 5, 5]), tensor([9, 1]), tensor([2, 9, 2, 1]), tensor([9, 1, 2, 2, 1]), tensor([9]), tensor([9]), tensor([9, 9]), tensor([2, 1, 5, 5]), tensor([2, 5, 1, 2, 9]), tensor([2, 9, 3, 2, 7]), tensor([3, 1, 2, 9]), tensor([9, 2, 1]), tensor([2, 2, 9]), tensor([2, 1]), tensor([2, 9, 2, 1]), tensor([2, 1, 5, 9, 2, 7]), tensor([3, 1, 2]), tensor([ 9,  1,  2,  3, 10,  3]), tensor([2, 9, 1, 2, 1, 3]), tensor([2, 2, 9]), tensor([2, 5, 9, 5]), tensor([2, 1]), tensor([5, 1, 2, 9]), tensor([2, 9, 3, 2, 7]), tensor([3, 1]), tensor([2, 1, 3]), tensor([2, 2, 3, 3]), tensor([3, 9]), tensor([ 9,  9, 12,  9,  2,  2,  1,  3]), tensor([2, 3, 1]), tensor([2, 2, 9, 2, 2, 9]), tensor([ 3,  2, 10,  5]), tensor([5, 1, 6]), tensor([2, 2, 9]), tensor([2, 1, 5, 9, 8]), tensor([5, 1, 3, 9]), tensor([2, 3, 2, 9, 2, 6]), tensor([2, 9, 2]), tensor([9]), tensor([9]), tensor([2, 1, 5, 5]), tensor([2, 9, 3, 2, 7]), tensor([3, 1, 2, 9]), tensor([2, 1]), tensor([2, 9, 3, 2, 7]), tensor([2, 2, 9]), tensor([2, 5, 9, 5]), tensor([2, 1]), tensor([5, 1, 2, 9]), tensor([3, 1, 2, 1, 3]), tensor([3, 1]), tensor([2, 9, 2, 1, 3]), tensor([ 3,  2, 10,  5]), tensor([2, 9]), tensor([ 5,  2, 10,  9]), tensor([2, 9, 7]), tensor([2, 2, 1, 5, 5]), tensor([2, 2, 9]), tensor([2, 5, 9, 5]), tensor([2, 1]), tensor([5, 1, 2, 9]), tensor([2, 9, 3, 2, 7]), tensor([3, 1]), tensor([2, 1, 3]), tensor([2, 2, 3, 3]), tensor([2, 2, 1, 2, 3, 9]), tensor([2, 1]), tensor([5, 2, 9, 2]), tensor([3, 1, 5]), tensor([2, 3, 2]), tensor([3, 9]), tensor([ 9,  1,  2,  3, 10,  9,  3]), tensor([9]), tensor([9]), tensor([2, 2, 2, 2, 9, 6]), tensor([2, 1, 5]), tensor([2, 1, 5, 9, 8]), tensor([2, 9, 3, 2, 6]), tensor([2, 9, 2]), tensor([2, 1, 5, 5, 1]), tensor([5, 1, 2, 5]), tensor([3, 1]), tensor([2, 1, 3]), tensor([2, 2, 9, 3]), tensor([9, 1, 3, 1, 3]), tensor([2, 2, 5]), tensor([2, 2, 9, 3]), tensor([ 5,  3, 11]), tensor([9, 1, 2, 1, 2, 3]), tensor([2, 9, 1, 2, 2, 9, 3, 3]), tensor([3, 1]), tensor([2, 2, 9, 3, 9, 2]), tensor([2, 2, 2, 9, 3, 2]), tensor([2, 2, 2, 9, 3, 3]), tensor([2, 1, 2, 9, 8]), tensor([2, 2, 1, 5, 5]), tensor([2, 3, 2, 1, 5]), tensor([3, 2, 9]), tensor([5, 1, 2, 9]), tensor([5, 3, 9, 5]), tensor([2, 1, 2, 5]), tensor([ 3,  2, 10]), tensor([9, 1, 2, 1, 2]), tensor([2, 1, 2, 5, 2, 9]), tensor([2, 1, 5, 9, 6]), tensor([3, 9]), tensor([ 2, 11]), tensor([ 9,  1,  9,  2,  9,  3, 11]), tensor([2, 1, 5, 5]), tensor([2, 2, 1]), tensor([9, 1]), tensor([ 2, 10,  2,  2,  9]), tensor([2, 1, 2]), tensor([2, 1, 3, 3]), tensor([2, 3, 1, 5, 5]), tensor([2, 2]), tensor([9, 1, 2, 1, 3]), tensor([2, 3, 1]), tensor([2, 9, 2]), tensor([ 9,  1,  2,  9,  2,  1,  3, 12]), tensor([ 2,  9,  9, 12,  9,  2,  9,  3, 12]), tensor([2, 1, 3, 9, 7, 9]), tensor([2, 2, 9]), tensor([2, 3, 2]), tensor([ 5,  2, 11,  5,  5]), tensor([9, 1, 2, 1, 2, 2]), tensor([3, 1]), tensor([3, 2, 9, 2, 3]), tensor([9, 1, 2, 2, 3]), tensor([2, 2, 9]), tensor([2, 1]), tensor([2, 1, 2, 2, 2, 9]), tensor([2, 2, 9, 3, 2]), tensor([ 2,  3, 10,  5]), tensor([ 2,  2,  2, 10,  5]), tensor([2, 9, 3, 5]), tensor([2, 2, 9, 3, 3]), tensor([9, 1, 2, 2, 3]), tensor([2, 1]), tensor([2, 3, 1]), tensor([2, 1]), tensor([2, 2, 9]), tensor([2, 5, 9, 5]), tensor([2, 1]), tensor([2, 5, 9, 2]), tensor([2, 1]), tensor([2, 1, 3]), tensor([9, 1, 2, 2, 9, 3]), tensor([2, 5, 1, 3, 2]), tensor([2, 1]), tensor([2, 1, 8]), tensor([ 2, 10]), tensor([2, 2, 1, 3]), tensor([ 3,  2, 10,  2,  9]), tensor([ 2, 10,  9,  2,  6]), tensor([2, 9, 3, 2, 6]), tensor([2, 2, 1]), tensor([2, 5, 1]), tensor([3, 9, 2, 9]), tensor([2, 2, 9, 3, 3]), tensor([ 9,  1,  2, 10,  9,  2,  9,  3]), tensor([2, 1]), tensor([2, 2, 2]), tensor([2, 2, 9, 2, 6]), tensor([2, 1, 7, 9]), tensor([ 3,  9,  9, 11,  1,  3]), tensor([3, 2, 1, 2, 5, 9, 2, 9]), tensor([ 3,  9,  9, 10,  3,  2,  3]), tensor([2, 2, 9, 2]), tensor([2, 2, 5, 5, 1]), tensor([2, 2, 2, 2]), tensor([9, 1, 2, 9, 3]), tensor([2, 1, 2]), tensor([9, 1, 2, 1, 3]), tensor([ 5, 14,  9]), tensor([2, 2, 2, 2]), tensor([ 9,  9, 10,  9,  3,  5]), tensor([2, 2, 2, 2]), tensor([2, 1, 2]), tensor([9, 1, 2, 1, 3]), tensor([2, 3, 9, 7, 9]), tensor([2, 2, 1]), tensor([2, 9, 2]), tensor([2, 3, 2]), tensor([3, 3, 1, 3]), tensor([ 5, 11]), tensor([2, 9, 3, 8]), tensor([3, 1, 5, 1]), tensor([ 2,  9,  1,  5, 12]), tensor([2, 1, 2, 5, 1]), tensor([2, 2, 1, 3, 5]), tensor([ 2, 12]), tensor([ 9,  1,  5, 11]), tensor([3, 5, 5, 3]), tensor([ 9,  1,  5, 11]), tensor([2, 2, 9]), tensor([ 9,  9, 10,  9,  3,  3]), tensor([ 2,  9,  9, 10,  3,  9,  5]), tensor([2, 1, 2, 5, 1]), tensor([2, 1, 7, 9]), tensor([2, 1]), tensor([2, 1, 3, 5]), tensor([5, 2]), tensor([2, 1, 7, 9]), tensor([2, 2, 1, 3, 2, 9]), tensor([3, 5, 5, 2, 1]), tensor([ 9,  9, 10,  3,  9,  3]), tensor([ 9, 10,  3,  9]), tensor([2, 2, 2]), tensor([2, 1, 2]), tensor([2, 1, 3]), tensor([2, 3, 9, 3]), tensor([ 2, 11]), tensor([9, 1, 2, 1, 3]), tensor([2, 2, 1]), tensor([2, 2, 5, 9, 9]), tensor([3, 2, 9]), tensor([9, 1, 8]), tensor([ 2,  9,  1,  5, 12]), tensor([5, 5, 5, 5]), tensor([ 3,  2, 11]), tensor([9, 1, 2, 1, 3]), tensor([2, 5, 5, 2, 9, 5]), tensor([2, 5, 2, 9]), tensor([2, 2, 1, 2, 2, 9, 3]), tensor([5, 1, 2, 9, 2]), tensor([5, 5, 3]), tensor([9, 1, 2, 1, 3]), tensor([ 2,  2, 10,  9]), tensor([ 2, 12]), tensor([ 9,  9, 10,  3,  9,  3]), tensor([2, 2, 1, 2, 5]), tensor([2, 1, 2, 9, 3, 3]), tensor([2, 5]), tensor([ 2,  3,  2, 11]), tensor([9, 1, 2, 1, 3]), tensor([5, 5, 5, 5]), tensor([ 3,  2, 10]), tensor([5, 5, 2, 1]), tensor([ 9,  9, 10,  3,  9,  3]), tensor([5, 3, 2]), tensor([2, 1, 7, 9]), tensor([2, 5, 2]), tensor([2, 5]), tensor([2, 5, 5, 5, 5]), tensor([5, 3, 2]), tensor([ 3,  2, 15]), tensor([2, 2, 2, 1, 3]), tensor([5, 1, 2, 9, 5]), tensor([9, 1, 2, 1, 3]), tensor([3, 9]), tensor([2, 1, 3]), tensor([5, 5, 2, 1]), tensor([2, 3, 2]), tensor([2, 2, 9]), tensor([3, 9]), tensor([9, 1, 5, 5, 9, 2, 1, 3]), tensor([3, 1]), tensor([2, 1, 2, 9, 3]), tensor([9, 1, 5, 2, 9, 2, 2, 3])]\n",
      "tensor([[2, 1, 5,  ..., 0, 0, 0],\n",
      "        [2, 1, 0,  ..., 0, 0, 0],\n",
      "        [2, 2, 2,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [3, 1, 0,  ..., 0, 0, 0],\n",
      "        [2, 1, 2,  ..., 0, 0, 0],\n",
      "        [9, 1, 5,  ..., 2, 3, 0]])\n"
     ]
    }
   ],
   "source": [
    "X_train_padded = [torch.tensor([char_to_index[char] for char in word]) for sentence in X_train for word in sentence ]\n",
    "X_train_padded = pad_sequence(X_train_padded, batch_first=True)\n",
    "\n",
    "y_train_padded = [torch.tensor([diacritic_to_index[char] for char in word]) for sentence in y_train for word in sentence ]\n",
    "print(y_train_padded)\n",
    "y_train_padded = pad_sequence(y_train_padded, batch_first=True)\n",
    "print(y_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(39, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=3, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=RNN(len(unique_characters)+1,len(unique_diacritics),embedding_dim=200,hidden_size=256,num_layers=3)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [394, 15], got [394, 9]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [79]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, train_labels, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_input)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# (7) loss calculation (you need to think in this part how to calculate the loss correctly)\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# (8) append the batch loss to the total_loss_train\u001b[39;00m\n\u001b[0;32m     49\u001b[0m total_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [394, 15], got [394, 9]"
     ]
    }
   ],
   "source": [
    "train(model,X_train_padded,y_train_padded,batch_size=512,epochs=5,learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

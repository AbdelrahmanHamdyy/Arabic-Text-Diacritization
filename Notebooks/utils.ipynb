{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Char [['m', 's', '>', 'l', 'p'], ['w', 'm', 'n'], ['H', 'n', 'v'], ['w', 'h', 'w'], ['q', 'A', 'd', 'r'], ['E', 'l', 'Y'], ['A', 'l', '<', 'T', 'E', 'A', 'm'], ['>', 'w'], ['A', 'l', 'k', 's', 'w', 'p'], ['>', 'w'], ['A', 'l', 'E', 't', 'q'], ['v', 'm'], ['A', 'f', 't', 'q', 'r'], ['f', 'E', 'j', 'z'], ['E', 'n'], ['k', 'l'], ['*', 'l', 'k'], ['l', 'm'], ['y', 'j', 'z', 'h'], ['A', 'l', 'S', 'w', 'm'], ['>', 'S', 'l', 'A']]\n",
      "Diac [['a', 'o', 'a', 'a', 'N'], ['a', 'a', 'o'], ['a', 'i', 'a'], ['a', 'u', 'a'], ['a', ' ', 'i', 'N'], ['a', 'a', ' '], [' ', ' ', 'i', 'o', 'a', ' ', 'i'], ['a', 'o'], [' ', 'o', 'i', 'o', 'a', 'i'], ['a', 'o'], [' ', 'o', 'i', 'o', 'i'], ['u', '~a'], [' ', 'o', 'a', 'a', 'a'], ['a', 'a', 'a', 'a'], ['a', 'o'], ['u', '~i'], ['a', 'i', 'a'], ['a', 'o'], ['u', 'o', 'i', 'i'], [' ', ' ', '~a', 'o', 'u'], ['a', 'o', 'F', ' ']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique characters and diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "unique_characters = ['A', 'b', 't', 'v', 'j', 'H', 'x', 'd', '*', 'r', 'z', 's', '$', 'S', 'D', 'T', 'Z', 'E', 'g', 'f', 'q', 'k', 'l', 'm', 'n', 'h', 'w', 'y', \"'\", '>', '<', '&', '}', '|', '{', '`', 'Y', 'p']\n",
    "unique_diacritics = ['o', 'a', 'i', '~', 'u', 'N', 'F', 'K', ' ', '~a', '~i', '~u', '~N', '~F', '~K']\n",
    "\n",
    "num_chars = len(unique_characters)\n",
    "num_classes = len(unique_diacritics)\n",
    "\n",
    "char_to_index = {char: i for i, char in enumerate(unique_characters)}\n",
    "diacritic_to_index = {diacritic: i for i, diacritic in enumerate(unique_diacritics)}\n",
    "\n",
    "print(num_chars)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path):\n",
    "\tsentences = []\n",
    "\twith open(path, 'r', encoding='utf-8') as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tsentences.append(line.strip())\n",
    "\n",
    "\treturn sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../dataset/train.txt\"\n",
    "VAL_PATH = \"../dataset/val.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_data():\n",
    "\tcorpus = readFile(TRAIN_PATH)\n",
    "\tloaded_model = FastText.load(\"./models/ft_model\")\n",
    "\tX_train = []\n",
    "\ty_train = []\n",
    "\tembeddings_train = []\n",
    "\tmax_sequence_length = 0\n",
    "\n",
    "\tfor sentence in corpus:\n",
    "\t\t# Clean each sentence in the corpus\n",
    "\t\tclean_sentence = run_buckwalter(sentence.strip())\n",
    "\t\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\t\tchar_list, diacritics_list = extract_labels(clean_sentence)\n",
    "\n",
    "\t\tX_train.append(char_list)\n",
    "\t\ty_train.append(diacritics_list)\n",
    "\n",
    "\t\t# Get the max sequence length and concatenate the embeddings of the words\n",
    "\t\tfor word in char_list:\n",
    "\t\t\tmax_sequence_length = max(max_sequence_length, len(word))\n",
    "\n",
    "\t\t\tembeddings_train.append(loaded_model.wv[word])\n",
    "\n",
    "\t# embeddings_train = np.concatenate(embeddings_train, axis=0)\n",
    "\tprint(np.array(embeddings_train).shape)\n",
    "\treturn X_train, y_train, embeddings_train, max_sequence_length\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_data( X_train, y_train,max_sequence_length):\n",
    "\t# Encoding and Padding the data\n",
    "\tX_train_padded = []\n",
    "\tfor sentence in X_train:\n",
    "\t\tX_train_sequences = [[char_to_index[char] for char in word] for word in sentence]\n",
    "\t\tX_train_padded.append(pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "\t# X_train_padded = np.concatenate(X_train_padded, axis=0)\n",
    "\tprint(np.array(X_train_padded).shape)\n",
    "\n",
    "\ty_train_padded = []\n",
    "\tfor sentence in y_train:\n",
    "\t\ty_train_sequences = [[diacritic_to_index[diacritic] for diacritic in diacritic_sequence] for diacritic_sequence in sentence]\n",
    "\t\ty_train_padded.append(pad_sequences(y_train_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "\t# y_train_padded = np.concatenate(y_train_padded, axis=0)\n",
    "\tprint(np.array(y_train_padded).shape)\n",
    "\treturn X_train_padded, y_train_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_val():\n",
    "    val_corpus = readFile(VAL_PATH)\n",
    "    loaded_model = FastText.load(\"./models/ft_model\")\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    embeddings_train = []\n",
    "    for sentence in val_corpus:\n",
    "        # Clean each sentence in the corpus\n",
    "        clean_sentence = run_buckwalter(sentence.strip())\n",
    "        # Get the char list for each word in the sentence and its corresponding diacritics\n",
    "        char_list, diacritics_list = extract_labels(clean_sentence)\n",
    "\n",
    "        X_val.append(char_list)\n",
    "        y_val.append(diacritics_list)\n",
    "        for word in char_list:\n",
    "\n",
    "            embeddings_train.append(loaded_model.wv[word])\n",
    "    return X_val, y_val,embeddings_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_val(X_val, y_val, max_sequence_length):\n",
    "    # Encoding and Padding the data\n",
    "    X_val_padded = []\n",
    "    for sentence in X_val:\n",
    "        X_val_sequences = [[char_to_index[char] for char in word] for word in sentence]\n",
    "        X_val_padded.append(pad_sequences(X_val_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "    # X_val_padded = np.concatenate(X_val_padded, axis=0)\n",
    "    print(np.array(X_val_padded).shape)\n",
    "\n",
    "    y_val_padded = []\n",
    "    for sentence in y_val:\n",
    "        y_val_sequences = [[diacritic_to_index[diacritic] for diacritic in diacritic_sequence] for diacritic_sequence in sentence]\n",
    "        y_val_padded.append(pad_sequences(y_val_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "    # y_val_padded = np.concatenate(y_val_padded, axis=0)\n",
    "    print(np.array(y_val_padded).shape)\n",
    "    return X_val_padded, y_val_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

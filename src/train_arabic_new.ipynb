{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ق', 'و', 'ل', 'ه'], ['ل', 'ع', 'د', 'م'], ['م', 'ا'], ['ت', 'ت', 'ع', 'ل', 'ق'], ['إ', 'ل', 'خ'], ['أ', 'ي'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['ق', 'و', 'ل', 'ه'], ['م', 'ا'], ['م', 'ر'], ['أ', 'ي'], ['ق', 'ب', 'ي', 'ل'], ['ق', 'و', 'ل'], ['ا', 'ل', 'م', 'ت', 'ن'], ['ل', 'غ', 'ت'], ['و', 'ل', 'و'], ['ا', 'ق', 'ت', 'ص', 'ر'], ['ع', 'ل', 'ى'], ['أ', 'و', 'ص', 'ي', 'ت'], ['ل', 'ه'], ['ب', 'ش', 'ا', 'ة'], ['أ', 'و'], ['أ', 'ع', 'ط', 'و', 'ه'], ['ش', 'ا', 'ة'], ['و', 'ل', 'ا'], ['غ', 'ن', 'م'], ['ل', 'ه'], ['ع', 'ن', 'د'], ['ا', 'ل', 'م', 'و', 'ت'], ['ه', 'ل'], ['ت', 'ب', 'ط', 'ل'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['أ', 'و'], ['ي', 'ش', 'ت', 'ر', 'ى'], ['ل', 'ه'], ['ش', 'ا', 'ة'], ['و', 'ي', 'ؤ', 'خ', 'ذ'], ['م', 'ن'], ['ق', 'و', 'ل', 'ه'], ['ا', 'ل', 'آ', 'ت', 'ي'], ['ك', 'م', 'ا'], ['ل', 'و'], ['ل', 'م'], ['ي', 'ق', 'ل'], ['م', 'ن'], ['م', 'ا', 'ل', 'ي'], ['و', 'ل', 'ا'], ['م', 'ن'], ['غ', 'ن', 'م', 'ي'], ['أ', 'ن', 'ه', 'ا'], ['ل', 'ا'], ['ت', 'ب', 'ط', 'ل'], ['و', 'ع', 'ب', 'ا', 'ر', 'ة'], ['ا', 'ل', 'ك', 'ن', 'ز'], ['و', 'ل', 'و'], ['ل', 'م'], ['ي', 'ق', 'ل'], ['م', 'ن'], ['م', 'ا', 'ل', 'ي'], ['و', 'ل', 'ا'], ['م', 'ن'], ['غ', 'ن', 'م', 'ي'], ['ل', 'م'], ['ي', 'ت', 'ع', 'ي', 'ن'], ['غ', 'ن', 'م', 'ه'], ['إ', 'ن'], ['ك', 'ا', 'ن', 'ت'], ['ا', 'ن', 'ت', 'ه', 'ت'], [], [], ['س', 'م'], ['ق', 'و', 'ل', 'ه'], ['ف', 'ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن', 'ه', 'ا'], ['إ', 'ل', 'خ'], ['ك', 'م', 'ا'], ['ل', 'و'], ['ك', 'ا', 'ن', 'ت'], ['م', 'و', 'ج', 'و', 'د', 'ة'], ['ع', 'ن', 'د'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['و', 'ا', 'ل', 'م', 'و', 'ت'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن'], ['غ', 'ي', 'ر'], ['غ', 'ن', 'م', 'ه'], ['ف', 'ي'], ['ا', 'ل', 'ص', 'و', 'ر', 'ت', 'ي', 'ن'], ['و', 'إ', 'ن'], ['ت', 'ر', 'ا', 'ض', 'ي', 'ا'], ['ل', 'أ', 'ن', 'ه'], ['ص', 'ل', 'ح'], ['ع', 'ل', 'ى'], ['م', 'ج', 'ه', 'و', 'ل'], ['م', 'غ', 'ن', 'ي'], ['و', 'ن', 'ه', 'ا', 'ي', 'ة'], ['ق', 'ا', 'ل'], [], [], ['ق', 'و', 'ل', 'ه'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن', 'ه', 'ا'], ['أ', 'ي'], ['ك', 'ا', 'م', 'ل', 'ة'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['ن', 'ص', 'ف', 'ي', 'ن'], ['م', 'ن'], ['ش', 'ا', 'ت', 'ي', 'ن'], ['ل', 'أ', 'ن', 'ه'], ['ل', 'ا'], ['ي', 'س', 'م', 'ى'], ['ش', 'ا', 'ة'], ['و', 'ق', 'و', 'ل', 'ه'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن'], ['غ', 'ي', 'ر'], ['غ', 'ن', 'م', 'ه'], ['و', 'ي', 'ن', 'ب', 'غ', 'ي'], ['أ', 'ن'], ['ي', 'ق', 'ا', 'ل'], ['م', 'ث', 'ل'], ['ذ', 'ل', 'ك'], ['ف', 'ي'], ['ا', 'ل', 'أ', 'ر', 'ق', 'ا', 'ء'], [], []]\n",
      "[['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'َ', 'ِ'], ['َ', ' '], ['َ', 'َ', 'َ', 'َّ', 'ُ'], [' ', 'َ', 'ْ'], ['َ', 'ْ'], [' ', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', ' '], ['َ', 'َّ'], ['َ', 'ْ'], ['ُ', 'َ', 'ْ', 'َ'], ['َ', 'ْ', 'ِ'], [' ', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'َ', 'ْ'], [' ', 'ْ', 'َ', 'َ', 'َ'], ['َ', 'َ', ' '], ['َ', 'ْ', 'َ', 'ْ', ' '], ['َ', 'ُ'], ['ِ', 'َ', ' ', 'ٍ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', ' ', 'ُ'], ['َ', ' ', 'ً'], ['َ', 'َ', ' '], ['َ', 'َ', 'َ'], ['َ', 'ُ'], ['ِ', 'ْ', 'َ'], [' ', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', 'ُ'], [' ', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', ' '], ['َ', 'ُ'], ['َ', ' ', 'ٌ'], ['َ', 'ُ', 'ْ', 'َ', 'ُ'], ['ِ', 'ْ'], ['َ', 'ْ', 'ِ', 'ِ'], [' ', 'ْ', ' ', 'ِ', ' '], ['َ', 'َ', ' '], ['َ', 'ْ'], ['َ', 'ْ'], ['َ', 'ُ', 'ْ'], ['ِ', 'ْ'], ['َ', ' ', 'ِ', ' '], ['َ', 'َ', ' '], ['ِ', 'ْ'], ['َ', 'َ', 'ِ', ' '], ['َ', 'َّ', 'َ', ' '], ['َ', ' '], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ِ', 'َ', ' ', 'َ', 'ُ'], [' ', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'ْ'], ['َ', 'ُ', 'ْ'], ['ِ', 'ْ'], ['َ', ' ', 'ِ', ' '], ['َ', 'َ', ' '], ['ِ', 'ْ'], ['َ', 'َ', 'ِ', ' '], ['َ', 'ْ'], ['َ', 'َ', 'َ', 'َّ', 'ْ'], ['َ', 'َ', 'ُ', 'ُ'], [' ', 'ْ'], ['َ', ' ', 'َ', 'ْ'], [' ', 'ْ', 'َ', 'َ', 'ْ'], [], [], [' ', ' '], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ُ', 'ْ', 'َ', ' '], ['َ', ' ', 'ِ', 'َ', 'ً'], ['ِ', 'ْ', 'َ', ' '], [' ', 'َ', 'ْ'], ['َ', 'َ', ' '], ['َ', 'ْ'], ['َ', ' ', 'َ', 'ْ'], ['َ', 'ْ', 'ُ', ' ', 'َ', 'ً'], ['ِ', 'ْ', 'َ'], [' ', 'ْ', 'َ', 'ِ', 'َّ', 'ِ'], ['َ', ' ', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', ' '], ['َ', 'ُ', ' ', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', ' '], ['َ', ' ', 'ِ', 'َ', 'ً'], ['ِ', 'ْ'], ['َ', 'ْ', 'ِ'], ['َ', 'َ', 'ِ', 'ِ'], ['ِ', ' '], [' ', ' ', 'ُّ', ' ', 'َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'ْ'], ['َ', 'َ', ' ', 'َ', 'َ', ' '], ['ِ', 'َ', 'َّ', 'ُ'], ['ُ', 'ْ', 'ٌ'], ['َ', 'َ', ' '], ['َ', 'ْ', 'ُ', ' ', 'ٍ'], ['ُ', 'ْ', 'ِ', ' '], ['َ', 'ِ', 'َ', ' ', 'َ', 'ٌ'], ['َ', ' ', 'َ'], [], [], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', ' ', 'ِ', 'َ', 'ً'], ['ِ', 'ْ', 'َ', ' '], ['َ', 'ْ'], ['َ', ' ', 'ِ', 'َ', 'ً'], ['َ', 'َ', ' '], ['َ', 'ُ', ' ', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', ' '], ['ِ', 'ْ', 'َ', 'ْ', 'ِ'], ['ِ', 'ْ'], ['َ', ' ', 'َ', 'ْ', 'ِ'], ['ِ', 'َ', 'َّ', 'ُ'], ['َ', ' '], ['ُ', 'َ', 'َّ', ' '], ['َ', ' ', 'ً'], ['َ', 'َ', 'ْ', 'ُ', 'ُ'], ['َ', 'َ', ' '], ['َ', 'ُ', ' ', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', ' '], ['َ', ' ', 'ِ', 'َ', 'ً'], ['ِ', 'ْ'], ['َ', 'ْ', 'ِ'], ['َ', 'َ', 'ِ', 'ِ'], ['َ', 'َ', 'ْ', 'َ', 'ِ', ' '], ['َ', 'ْ'], ['ُ', 'َ', ' ', 'َ'], ['ِ', 'ْ', 'ُ'], ['َ', 'ِ', 'َ'], ['ِ', ' '], [' ', 'ْ', 'َ', 'ِ', 'َّ', ' ', 'ِ'], [], []]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from TorchCRF import CRF\n",
    "\n",
    "%run updatePreprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "VOCAB_SIZE = len(basic_arabic_letters) + 1\n",
    "LABELS_SIZE = len(DIACRITICS)\n",
    "\n",
    "TRAIN_PATH \t= \"../dataset/train.txt\"\n",
    "VAL_PATH \t\t= \"../dataset/val.txt\"\n",
    "RNN_PATH\t\t= \"./models/rnn.pth\"\n",
    "CNN_PATH \t\t= \"./models/cnn.pth\"\n",
    "CRF_PATH \t\t= \"./models/crf.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, n_classes=LABELS_SIZE, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        \"\"\"\n",
    "        The constructor of our RNN model\n",
    "        Inputs:\n",
    "        - vacab_size: the number of unique characters\n",
    "        - embedding_dim: the embedding dimension\n",
    "        - n_classes: the number of final classes (diacritics)\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # (1) Create an embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # (3) Create a linear layer with number of neorons = n_classes\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        \"\"\"\n",
    "        This function does the forward pass of our model\n",
    "        Inputs:\n",
    "        - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "        Returns:\n",
    "        - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # final_output = None\n",
    "        \n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        # final_output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, num_classes=LABELS_SIZE, embedding_dim=EMBEDDING_DIM, filter_sizes=[3, 4, 5], num_filters=[100, 100, 100], dropout=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        \n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
    "        x_embed = self.embedding(input_ids).float()\n",
    "\n",
    "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
    "        # Output shape: (b, embed_dim, max_len)\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "        \n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "        \n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, n_classes=LABELS_SIZE, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        super(LSTM_CRF, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(n_classes)  # Place the CRF layer after the linear layer\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output  # Return raw output for CRF loss calculation\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        output = self.forward(sentences)\n",
    "        predictions = self.crf.decode(output)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=BATCH_SIZE):\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(path, model, optimizer, train_dataloader, val_dataloader=None, epochs=NUM_EPOCHS):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    \n",
    "    # Tracking best validation accuracy\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "\n",
    "        # Tracking time and loss\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass\n",
    "            output = model(b_input_ids)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(output.view(-1, output.shape[-1]), b_labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if val_dataloader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Track the best accuracy\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), path)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"\n",
    "    After the completion of each training epoch, measure the model's\n",
    "    performance on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Get the output\n",
    "        with torch.no_grad():\n",
    "            output = model(b_input_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(output.view(-1, output.shape[-1]), b_labels.view(-1))\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = output.argmax(dim=2)\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilize_model(model_type, learning_rate=LEARNING_RATE):\n",
    "    # Instantiate model\n",
    "    if model_type == \"CNN\":\n",
    "        model = CNN()\n",
    "        path = CNN_PATH\n",
    "    elif model_type == \"RNN\":\n",
    "        model = RNN()\n",
    "        path = RNN_PATH\n",
    "    elif model_type == \"CRF\":\n",
    "        model = LSTM_CRF()\n",
    "        path = CRF_PATH\n",
    "\n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return path, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path):\n",
    "\tsentences = []\n",
    "\twith open(path, 'r', encoding='utf-8') as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tsentences.append(line.strip())\n",
    "\n",
    "\treturn sentences\n",
    "\n",
    "train_corpus = readFile(TRAIN_PATH)\n",
    "val_corpus = readFile(VAL_PATH)\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for sentence in train_corpus[:100]:\n",
    "\t# Clean each sentence in the corpus\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\tchar_list, diacritics_list = separate_words_and_diacritics(sentence.strip())\n",
    "\n",
    "\tX_train.append(char_list)\n",
    "\tY_train.append(diacritics_list)\n",
    "\n",
    "X_train_padded = [torch.tensor([char_to_index[char] for char in word]) for sentence in X_train for word in sentence]\n",
    "X_train_padded = pad_sequence(X_train_padded, batch_first=True)\n",
    "\n",
    "y_train_padded = [torch.tensor([diacritic_to_index[char] for char in word]) for sentence in Y_train for word in sentence]\n",
    "y_train_padded = pad_sequence(y_train_padded, batch_first=True)\n",
    "\n",
    "\n",
    "for sentence in val_corpus:\n",
    "\t# Clean each sentence in the corpus\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\tchar_list, diacritics_list = separate_words_and_diacritics(sentence.strip())\n",
    "\n",
    "\tX_val.append(char_list)\n",
    "\ty_val.append(diacritics_list)\n",
    "\n",
    "X_val_padded = [torch.tensor([char_to_index[char] for char in word]) for sentence in X_val for word in sentence ]\n",
    "X_val_padded = pad_sequence(X_val_padded, batch_first=True)\n",
    "\n",
    "y_val_padded = [torch.tensor([diacritic_to_index[char] for char in word]) for sentence in y_val for word in sentence ]\n",
    "y_val_padded = pad_sequence(y_val_padded, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   0.848374   |  0.527716  |   81.37   |   31.55  \n",
      "   2    |   0.481270   |  0.446679  |   84.39   |   31.06  \n",
      "   3    |   0.404675   |  0.377792  |   86.87   |   30.88  \n",
      "   4    |   0.336478   |  0.321772  |   89.18   |   31.46  \n",
      "   5    |   0.285216   |  0.287496  |   89.99   |   30.91  \n",
      "   6    |   0.249742   |  0.262634  |   91.00   |   31.33  \n",
      "   7    |   0.224075   |  0.245662  |   91.55   |   31.85  \n",
      "   8    |   0.199930   |  0.233454  |   92.15   |   31.94  \n",
      "   9    |   0.182190   |  0.223623  |   92.50   |   31.07  \n",
      "  10    |   0.165331   |  0.217065  |   92.69   |   31.28  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 92.69%.\n"
     ]
    }
   ],
   "source": [
    "path, model, optimizer = initilize_model(\"RNN\")\n",
    "train_dataloader, val_dataloader = data_loader(X_train_padded, X_val_padded, y_train_padded, y_val_padded)\n",
    "train(path, model, optimizer, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

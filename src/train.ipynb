{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n",
      "['ق', 'و', 'ل', 'ه', 'ل', 'ع', 'د', 'م', 'م', 'ا', 'ت', 'ت', 'ع', 'ل', 'ق', 'إ', 'ل', 'خ', 'أ', 'ي', 'ا', 'ل', 'و', 'ص', 'ي', 'ة', 'ق', 'و', 'ل', 'ه', 'م', 'ا', 'م', 'ر', 'أ', 'ي', 'ق', 'ب', 'ي', 'ل', 'ق', 'و', 'ل', 'ا', 'ل', 'م', 'ت', 'ن', 'ل', 'غ', 'ت', 'و', 'ل', 'و', 'ا', 'ق', 'ت', 'ص', 'ر', 'ع', 'ل', 'ى', 'أ', 'و', 'ص', 'ي', 'ت', 'ل', 'ه', 'ب', 'ش', 'ا', 'ة', 'أ', 'و', 'أ', 'ع', 'ط', 'و', 'ه', 'ش', 'ا', 'ة', 'و', 'ل', 'ا', 'غ', 'ن', 'م', 'ل', 'ه', 'ع', 'ن', 'د', 'ا', 'ل', 'م', 'و', 'ت', 'ه', 'ل', 'ت', 'ب', 'ط', 'ل', 'ا', 'ل', 'و', 'ص', 'ي', 'ة', 'أ', 'و', 'ي', 'ش', 'ت', 'ر', 'ى', 'ل', 'ه', 'ش', 'ا', 'ة', 'و', 'ي', 'ؤ', 'خ', 'ذ', 'م', 'ن', 'ق', 'و', 'ل', 'ه', 'ا', 'ل', 'آ', 'ت', 'ي', 'ك', 'م', 'ا', 'ل', 'و', 'ل', 'م', 'ي', 'ق', 'ل', 'م', 'ن', 'م', 'ا', 'ل', 'ي', 'و', 'ل', 'ا', 'م', 'ن', 'غ', 'ن', 'م', 'ي', 'أ', 'ن', 'ه', 'ا', 'ل', 'ا', 'ت', 'ب', 'ط', 'ل', 'و', 'ع', 'ب', 'ا', 'ر', 'ة', 'ا', 'ل', 'ك', 'ن', 'ز', 'و', 'ل', 'و', 'ل', 'م', 'ي', 'ق', 'ل', 'م', 'ن', 'م', 'ا', 'ل', 'ي', 'و', 'ل', 'ا', 'م', 'ن', 'غ', 'ن', 'م', 'ي', 'ل', 'م', 'ي', 'ت', 'ع', 'ي', 'ن', 'غ', 'ن', 'م', 'ه', 'إ', 'ن', 'ك', 'ا', 'ن', 'ت', 'ا', 'ن', 'ت', 'ه', 'ت', 'س', 'م', 'ق', 'و', 'ل', 'ه', 'ف', 'ي', 'ع', 'ط', 'ى', 'و', 'ا', 'ح', 'د', 'ة', 'م', 'ن', 'ه', 'ا', 'إ', 'ل', 'خ', 'ك', 'م', 'ا', 'ل', 'و', 'ك', 'ا', 'ن', 'ت', 'م', 'و', 'ج', 'و', 'د', 'ة', 'ع', 'ن', 'د', 'ا', 'ل', 'و', 'ص', 'ي', 'ة', 'و', 'ا', 'ل', 'م', 'و', 'ت', 'و', 'ل', 'ا', 'ي', 'ج', 'و', 'ز', 'أ', 'ن', 'ي', 'ع', 'ط', 'ى', 'و', 'ا', 'ح', 'د', 'ة', 'م', 'ن', 'غ', 'ي', 'ر', 'غ', 'ن', 'م', 'ه', 'ف', 'ي', 'ا', 'ل', 'ص', 'و', 'ر', 'ت', 'ي', 'ن', 'و', 'إ', 'ن', 'ت', 'ر', 'ا', 'ض', 'ي', 'ا', 'ل', 'أ', 'ن', 'ه', 'ص', 'ل', 'ح', 'ع', 'ل', 'ى', 'م', 'ج', 'ه', 'و', 'ل', 'م', 'غ', 'ن', 'ي', 'و', 'ن', 'ه', 'ا', 'ي', 'ة', 'ق', 'ا', 'ل', 'ق', 'و', 'ل', 'ه', 'و', 'ا', 'ح', 'د', 'ة', 'م', 'ن', 'ه', 'ا', 'أ', 'ي', 'ك', 'ا', 'م', 'ل', 'ة', 'و', 'ل', 'ا', 'ي', 'ج', 'و', 'ز', 'أ', 'ن', 'ي', 'ع', 'ط', 'ى', 'ن', 'ص', 'ف', 'ي', 'ن', 'م', 'ن', 'ش', 'ا', 'ت', 'ي', 'ن', 'ل', 'أ', 'ن', 'ه', 'ل', 'ا', 'ي', 'س', 'م', 'ى', 'ش', 'ا', 'ة', 'و', 'ق', 'و', 'ل', 'ه', 'و', 'ل', 'ا', 'ي', 'ج', 'و', 'ز', 'أ', 'ن', 'ي', 'ع', 'ط', 'ى', 'و', 'ا', 'ح', 'د', 'ة', 'م', 'ن', 'غ', 'ي', 'ر', 'غ', 'ن', 'م', 'ه', 'و', 'ي', 'ن', 'ب', 'غ', 'ي', 'أ', 'ن', 'ي', 'ق', 'ا', 'ل', 'م', 'ث', 'ل', 'ذ', 'ل', 'ك', 'ف', 'ي', 'ا', 'ل', 'أ', 'ر', 'ق', 'ا', 'ء']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from TorchCRF import CRF\n",
    "\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "VOCAB_SIZE = len(basic_arabic_letters) + 1\n",
    "LABELS_SIZE = len(DIACRITICS)\n",
    "\n",
    "TRAIN_PATH = \"../dataset/train.txt\"\n",
    "VAL_PATH = \"../dataset/val.txt\"\n",
    "RNN_PATH = \"./models/rnn.pth\"\n",
    "CNN_PATH = \"./models/cnn.pth\"\n",
    "CRF_PATH = \"./models/crf.pth\"\n",
    "CBHG_PATH = \"./models/cbhg.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"There are {torch.cuda.device_count()} GPU(s) available.\")\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_classes=LABELS_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The constructor of our RNN model\n",
    "        Inputs:\n",
    "        - vacab_size: the number of unique characters\n",
    "        - embedding_dim: the embedding dimension\n",
    "        - n_classes: the number of final classes (diacritics)\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # (1) Create an embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # (3) Create a linear layer with number of neorons = n_classes\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        \"\"\"\n",
    "        This function does the forward pass of our model\n",
    "        Inputs:\n",
    "        - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "        Returns:\n",
    "        - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        num_classes=LABELS_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        filter_sizes=[3, 4, 5],\n",
    "        num_filters=[100, 100, 100],\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=embedding_dim,\n",
    "                    out_channels=num_filters[i],\n",
    "                    kernel_size=filter_sizes[i],\n",
    "                )\n",
    "                for i in range(len(filter_sizes))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
    "        x_embed = self.embedding(input_ids).float()\n",
    "\n",
    "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
    "        # Output shape: (b, embed_dim, max_len)\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [\n",
    "            F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list\n",
    "        ]\n",
    "\n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list], dim=1)\n",
    "\n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CRF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_classes=LABELS_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "    ):\n",
    "        super(LSTM_CRF, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(n_classes)  # Place the CRF layer after the linear layer\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output  # Return raw output for CRF loss calculation\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        output = self.forward(sentences)\n",
    "        predictions = self.crf.decode(output)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBHG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prenet(nn.Module):\n",
    "    \"\"\"\n",
    "    A prenet is a collection of linear layers with dropout(0.5),\n",
    "    and RELU activation function\n",
    "    Args:\n",
    "    config: the hyperparameters object\n",
    "    in_dim (int): the input dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_dim: int, prenet_depth: List[int] = [256, 128], dropout: int = 0.5\n",
    "    ):\n",
    "        \"\"\"Initializing the prenet module\"\"\"\n",
    "        super().__init__()\n",
    "        in_sizes = [in_dim] + prenet_depth[:-1]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(in_size, out_size)\n",
    "                for (in_size, out_size) in zip(in_sizes, prenet_depth)\n",
    "            ]\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"Calculate forward propagation\n",
    "        Args:\n",
    "        inputs (batch_size, seqLen): the inputs to the prenet, the input shapes could\n",
    "        be different as it is being used in both encoder and decoder.\n",
    "        Returns:\n",
    "        Tensor: the output of  the forward propagation\n",
    "        \"\"\"\n",
    "        for linear in self.layers:\n",
    "            inputs = self.dropout(self.relu(linear(inputs)))\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class Highway(nn.Module):\n",
    "    \"\"\"Highway Networks were developed by (Srivastava et al., 2015)\n",
    "    to overcome the difficulty of training deep neural networks\n",
    "    (https://arxiv.org/abs/1507.06228).\n",
    "    Args:\n",
    "    in_size (int): the input size\n",
    "    out_size (int): the output size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \"\"\"\n",
    "        Initializing Highway networks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.H = nn.Linear(in_size, out_size)\n",
    "        self.H.bias.data.zero_()\n",
    "        self.T = nn.Linear(in_size, out_size)\n",
    "        self.T.bias.data.fill_(-1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"Calculate forward propagation\n",
    "        Args:\n",
    "        inputs (Tensor):\n",
    "        \"\"\"\n",
    "        H = self.relu(self.H(inputs))\n",
    "        T = self.sigmoid(self.T(inputs))\n",
    "        return H * T + inputs * (1.0 - T)\n",
    "\n",
    "\n",
    "class BatchNormConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    A nn.Conv1d followed by an optional activation function, and nn.BatchNorm1d\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        padding: int,\n",
    "        activation=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_dim,\n",
    "            out_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return self.bn(x)\n",
    "\n",
    "\n",
    "class CBHG(nn.Module):\n",
    "    \"\"\"The CBHG module (1-D Convolution Bank + Highway network + Bidirectional GRU)\n",
    "    was proposed by (Lee et al., 2017, https://www.aclweb.org/anthology/Q17-1026)\n",
    "    for a character-level NMT model.\n",
    "    It was adapted by (Wang et al., 2017) for building the Tacotron.\n",
    "    It is used in both the encoder and decoder  with different parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        K: int,\n",
    "        projections,\n",
    "        highway_layers: int = 4,\n",
    "    ):\n",
    "        \"\"\"Initializing the CBHG module\n",
    "        Args:\n",
    "        in_dim (int): the input size\n",
    "        out_dim (int): the output size\n",
    "        k (int): number of filters\n",
    "        projections: A list of integers representing the output sizes of convolutional projections.\n",
    "        highway_layers: Number of highway layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        # list of modules each one of them will be BatchNoemConv1\n",
    "        # and each of them has a kernel size ranging from 1 -> k\n",
    "        self.conv1d_banks = nn.ModuleList(\n",
    "            [\n",
    "                BatchNormConv1d(\n",
    "                    in_dim,\n",
    "                    in_dim,\n",
    "                    kernel_size=k,\n",
    "                    stride=1,\n",
    "                    padding=k // 2,\n",
    "                    activation=self.relu,\n",
    "                )\n",
    "                for k in range(1, K + 1)\n",
    "            ]\n",
    "        )\n",
    "        \"\"\"\n",
    "        kernel_size: The size of the window over which the maximum value is computed. It specifies the size of the pooling operation.\n",
    "        stride: The step size to slide the pooling window along the input. If not specified, it defaults to kernel_size.\n",
    "        padding: Zero-padding added to both sides of the input. Padding helps to control the spatial dimensions of the output.\n",
    "        \"\"\"\n",
    "        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
    "\n",
    "        in_sizes = [K * in_dim] + projections[:-1]\n",
    "        activations = [self.relu] * (len(projections) - 1) + [None]\n",
    "        self.conv1d_projections = nn.ModuleList(\n",
    "            [\n",
    "                BatchNormConv1d(\n",
    "                    in_size, out_size, kernel_size=3, stride=1, padding=1, activation=ac\n",
    "                )\n",
    "                for (in_size, out_size, ac) in zip(in_sizes, projections, activations)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.pre_highway = nn.Linear(projections[-1], in_dim, bias=False)\n",
    "        self.highways = nn.ModuleList([Highway(in_dim, in_dim) for _ in range(4)])\n",
    "\n",
    "        self.gru = nn.GRU(in_dim, out_dim, 1, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, inputs, input_lengths=None):\n",
    "        # (B, T_in, in_dim)\n",
    "        x = inputs\n",
    "        x = x.transpose(1, 2)\n",
    "        T = x.size(-1)\n",
    "\n",
    "        # (B, in_dim*K, T_in)\n",
    "        # Concat conv1d bank outputs\n",
    "        x = torch.cat([conv1d(x)[:, :, :T] for conv1d in self.conv1d_banks], dim=1)\n",
    "        assert x.size(1) == self.in_dim * len(self.conv1d_banks)\n",
    "        x = self.max_pool1d(x)[:, :, :T]\n",
    "\n",
    "        for conv1d in self.conv1d_projections:\n",
    "            x = conv1d(x)\n",
    "\n",
    "        # (B, T_in, in_dim)\n",
    "        # Back to the original shape\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        if x.size(-1) != self.in_dim:\n",
    "            x = self.pre_highway(x)\n",
    "\n",
    "        # Residual connection\n",
    "        x += inputs\n",
    "        for highway in self.highways:\n",
    "            x = highway(x)\n",
    "\n",
    "        if input_lengths is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, input_lengths, batch_first=True)\n",
    "\n",
    "        # (B, T_in, in_dim*2)\n",
    "        self.gru.flatten_parameters()\n",
    "        outputs, _ = self.gru(x)\n",
    "\n",
    "        if input_lengths is not None:\n",
    "            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class CBHGModel(nn.Module):\n",
    "    \"\"\"CBHG model implementation as described in the paper:\n",
    "     https://ieeexplore.ieee.org/document/9274427\n",
    "\n",
    "    Args:\n",
    "    inp_vocab_size (int): the number of the input symbols\n",
    "    targ_vocab_size (int): the number of the target symbols (diacritics)\n",
    "    embedding_dim (int): the embedding  size\n",
    "    use_prenet (bool): whether to use prenet or not\n",
    "    prenet_sizes (List[int]): the sizes of the prenet networks\n",
    "    cbhg_gru_units (int): the number of units of the CBHG GRU, which is the last\n",
    "    layer of the CBHG Model.\n",
    "    cbhg_filters (int): number of filters used in the CBHG module\n",
    "    cbhg_projections: projections used in the CBHG module\n",
    "\n",
    "    Returns:\n",
    "    diacritics Dict[str, Tensor]:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_vocab_size: int,\n",
    "        targ_vocab_size: int,\n",
    "        embedding_dim: int = 512,\n",
    "        use_prenet: bool = True,\n",
    "        prenet_sizes=[512, 256],\n",
    "        cbhg_gru_units: int = 512,\n",
    "        cbhg_filters: int = 16,\n",
    "        cbhg_projections=[128, 256],\n",
    "        post_cbhg_layers_units=[256, 256],\n",
    "        post_cbhg_use_batch_norm: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_prenet = use_prenet\n",
    "        self.embedding = nn.Embedding(inp_vocab_size, embedding_dim)\n",
    "        if self.use_prenet:\n",
    "            self.prenet = Prenet(embedding_dim, prenet_depth=prenet_sizes)\n",
    "\n",
    "        self.cbhg = CBHG(\n",
    "            prenet_sizes[-1] if self.use_prenet else embedding_dim,\n",
    "            cbhg_gru_units,\n",
    "            K=cbhg_filters,\n",
    "            projections=cbhg_projections,\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        post_cbhg_layers_units = [cbhg_gru_units] + post_cbhg_layers_units\n",
    "\n",
    "        for i in range(1, len(post_cbhg_layers_units)):\n",
    "            layers.append(\n",
    "                nn.LSTM(\n",
    "                    post_cbhg_layers_units[i - 1] * 2,\n",
    "                    post_cbhg_layers_units[i],\n",
    "                    bidirectional=True,\n",
    "                    batch_first=True,\n",
    "                )\n",
    "            )\n",
    "            if post_cbhg_use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(post_cbhg_layers_units[i] * 2))\n",
    "\n",
    "        self.post_cbhg_layers = nn.ModuleList(layers)\n",
    "        self.projections = nn.Linear(post_cbhg_layers_units[-1] * 2, targ_vocab_size)\n",
    "        self.post_cbhg_layers_units = post_cbhg_layers_units\n",
    "        self.post_cbhg_use_batch_norm = post_cbhg_use_batch_norm\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        lengths=None,\n",
    "        target=None,  # not required in this model\n",
    "    ):\n",
    "        \"\"\"Compute forward propagation\"\"\"\n",
    "\n",
    "        # src = [batch_size, src len]\n",
    "        # lengths = [batch_size]\n",
    "        # target = [batch_size, trg len]\n",
    "\n",
    "        embedding_out = self.embedding(src)\n",
    "        # embedding_out; [batch_size, src_len, embedding_dim]\n",
    "\n",
    "        cbhg_input = embedding_out\n",
    "        if self.use_prenet:\n",
    "            cbhg_input = self.prenet(embedding_out)\n",
    "\n",
    "            # cbhg_input = [batch_size, src_len, prenet_sizes[-1]]\n",
    "\n",
    "        outputs = self.cbhg(cbhg_input, lengths)\n",
    "\n",
    "        hn = torch.zeros((2, 2, 2))\n",
    "        cn = torch.zeros((2, 2, 2))\n",
    "\n",
    "        for i, layer in enumerate(self.post_cbhg_layers):\n",
    "            if isinstance(layer, nn.BatchNorm1d):\n",
    "                outputs = layer(outputs.permute(0, 2, 1))\n",
    "                outputs = outputs.permute(0, 2, 1)\n",
    "                continue\n",
    "            if i > 0:\n",
    "                outputs, (hn, cn) = layer(outputs, (hn, cn))\n",
    "            else:\n",
    "                outputs, (hn, cn) = layer(outputs)\n",
    "\n",
    "        predictions = self.projections(outputs)\n",
    "\n",
    "        # predictions = [batch_size, src len, targ_vocab_size]\n",
    "\n",
    "        # output = {\"diacritics\": predictions}\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(\n",
    "    train_inputs, val_inputs, train_labels, val_labels, batch_size=BATCH_SIZE\n",
    "):\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, sampler=train_sampler, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(\n",
    "    path, model, optimizer, train_dataloader, val_dataloader=None, epochs=NUM_EPOCHS\n",
    "):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "\n",
    "    # Tracking best validation accuracy\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(\n",
    "        f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\"\n",
    "    )\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "\n",
    "        # Tracking time and loss\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass\n",
    "            output = model(b_input_ids)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(output.view(-1, output.shape[-1]), b_labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if val_dataloader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Track the best accuracy\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), path)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(\n",
    "                f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\"\n",
    "            )\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"\n",
    "    After the completion of each training epoch, measure the model's\n",
    "    performance on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Get the output\n",
    "        with torch.no_grad():\n",
    "            output = model(b_input_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(output.view(-1, output.shape[-1]), b_labels.view(-1))\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = output.argmax(dim=2)\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilize_model(model_type, learning_rate=LEARNING_RATE):\n",
    "    # Instantiate the model\n",
    "    if model_type == \"CNN\":\n",
    "        model = CNN()\n",
    "        path = CNN_PATH\n",
    "    elif model_type == \"RNN\":\n",
    "        model = RNN()\n",
    "        path = RNN_PATH\n",
    "    elif model_type == \"CRF\":\n",
    "        model = LSTM_CRF()\n",
    "        path = CRF_PATH\n",
    "    elif model_type == \"CBHG\":\n",
    "        model = CBHGModel(\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            inp_vocab_size=VOCAB_SIZE,\n",
    "            targ_vocab_size=LABELS_SIZE,\n",
    "            use_prenet=False,\n",
    "            prenet_sizes=[512, 256],\n",
    "            cbhg_gru_units=256,\n",
    "            cbhg_filters=16,\n",
    "            cbhg_projections=[128, 256],\n",
    "            post_cbhg_layers_units=[256, 256],\n",
    "            post_cbhg_use_batch_norm=True,\n",
    "        )\n",
    "        path = CBHG_PATH\n",
    "\n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return path, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = readFile(TRAIN_PATH)\n",
    "val_corpus = readFile(VAL_PATH)\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "for sentence in train_corpus[:2]:\n",
    "    # Clean each sentence in the corpus\n",
    "    # Get the char list for each word in the sentence and its corresponding diacritics\n",
    "    char_list, diacritics_list = separate_words_and_diacritics(sentence.strip())\n",
    "\n",
    "    for i in range(len(char_list)):\n",
    "        X_train.append(char_list[i])\n",
    "        Y_train.append(diacritics_list[i])\n",
    "\n",
    "X_train_padded = [\n",
    "    torch.tensor([char_to_index[char] for char in sentence]) for sentence in X_train\n",
    "]\n",
    "X_train_padded = pad_sequence(X_train_padded, batch_first=True)\n",
    "\n",
    "y_train_padded = [\n",
    "    torch.tensor([diacritic_to_index[char] for char in sentence])\n",
    "    for sentence in Y_train\n",
    "]\n",
    "y_train_padded = pad_sequence(y_train_padded, batch_first=True)\n",
    "\n",
    "\n",
    "for sentence in val_corpus:\n",
    "    # Clean each sentence in the corpus\n",
    "    # Get the char list for each word in the sentence and its corresponding diacritics\n",
    "    char_list, diacritics_list = separate_words_and_diacritics(sentence.strip())\n",
    "\n",
    "    for i in range(len(char_list)):\n",
    "        X_val.append(char_list[i])\n",
    "        Y_val.append(diacritics_list[i])\n",
    "\n",
    "X_val_padded = [\n",
    "    torch.tensor([char_to_index[char] for char in sentence]) for sentence in X_val\n",
    "]\n",
    "X_val_padded = pad_sequence(X_val_padded, batch_first=True)\n",
    "\n",
    "y_val_padded = [\n",
    "    torch.tensor([diacritic_to_index[char] for char in sentence]) for sentence in Y_val\n",
    "]\n",
    "y_val_padded = pad_sequence(y_val_padded, batch_first=True)\n",
    "\n",
    "val_inputs, test_inputs, val_labels, test_labels = train_test_split(\n",
    "    X_val_padded, y_val_padded, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m path, model, optimizer \u001b[38;5;241m=\u001b[39m initilize_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m train_dataloader, val_dataloader \u001b[38;5;241m=\u001b[39m data_loader(X_train_padded, val_inputs, y_train_padded, val_labels)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 69\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(path, model, optimizer, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# =======================================\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m#               Evaluation\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# =======================================\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_dataloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# After the completion of each training epoch, measure the model's\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# performance on our validation set.\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Track the best accuracy\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[1;32mIn[42], line 102\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, val_dataloader)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Get the output\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 102\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m    105\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), b_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 34\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# final_output = None\u001b[39;00m\n\u001b[0;32m     33\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(sentences)\n\u001b[1;32m---> 34\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(lstm_out)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# final_output = F.softmax(output, dim=1)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path, model, optimizer = initilize_model(\"RNN\")\n",
    "train_dataloader, val_dataloader = data_loader(\n",
    "    X_train_padded, val_inputs, y_train_padded, val_labels\n",
    ")\n",
    "train(path, model, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.22198111241540083\n",
      "Test Accuracy: 92.53917212516103\n"
     ]
    }
   ],
   "source": [
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "test_loss, test_acc = evaluate(model, test_dataloader)\n",
    "print(\"Test Loss: {}\".format(test_loss))\n",
    "print(\"Test Accuracy: {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

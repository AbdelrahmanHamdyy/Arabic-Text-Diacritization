{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Choose a pre-trained model (e.g., 'camembert-base')\n",
    "model_name = 'camembert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Embedding:\n",
      "[0.07309218496084213, 0.09774976968765259, 0.0, 0.03212716430425644, 0.0, 0.06682439148426056, 0.0, 0.0, 0.0, 0.0, 0.09632889926433563, 0.0, 0.09819561243057251, 0.0, 0.0, 0.1389247179031372, 0.07470124214887619, 0.0, 0.004326615482568741, 0.14504173398017883, 0.0, 0.0, 0.05533716082572937, 0.10472816228866577, 0.0, 0.0, 0.07715540379285812, 0.06222289800643921, 0.0, 0.11101175844669342, 0.010983675718307495, 0.0459124781191349, 0.0, 0.0, 0.0, 0.015538503415882587, 0.110532745718956, 0.0, 0.0, 0.06878892332315445, 0.0, 0.0, 0.0, 0.01830488257110119, 0.0, 0.0, 0.0032827984541654587, 0.0, 0.03479265421628952, 0.06602971255779266, 0.0902828723192215, 0.0, 0.0, 0.0, 0.18203869462013245, 0.0, 0.16060897707939148, 0.0, 0.02231748215854168, 0.05186638981103897, 0.15985539555549622, 0.12441415339708328, 0.0, 0.034329526126384735, 0.0010992810130119324, 0.0, 0.07230260223150253, 0.007854122668504715, 0.0825631394982338, 0.0657130628824234, 0.0, 0.1500445008277893, 0.0416051484644413, 0.0, 0.0, 0.04606892168521881, 0.0022214464843273163, 0.031198903918266296, 0.0, 0.0052888644859194756, 0.0076126474887132645, 0.0, 0.12640255689620972, 0.0, 0.0, 0.0, 0.003272753208875656, 0.06219039857387543, 0.10730136930942535, 0.023878399282693863, 0.01370604895055294, 0.0, 0.06002974510192871, 0.12762603163719177, 0.0, 0.0888376459479332, 0.0, 0.12165769934654236, 0.0029072649776935577, 0.12602479755878448, 0.040656328201293945, 0.013135723769664764, 0.09192262589931488, 0.0009653791785240173, 0.05744383856654167, 0.023208895698189735, 0.0, 0.0, 0.0, 0.04910480976104736, 0.08446447551250458, 0.0, 0.0, 0.004011843353509903, 0.0, 0.05901380255818367, 0.0, 0.2003406286239624, 0.02756396308541298, 0.10871952772140503, 0.07210412621498108, 0.04936908930540085, 0.034386105835437775, 0.0, 0.08116881549358368, 0.0, 0.02667267620563507, 0.0060283467173576355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025393765419721603, 0.04383764788508415, 0.07238622009754181, 0.053248073905706406, 0.0, 0.009918217547237873, 0.06824398785829544, 0.0, 0.0007880609482526779, 0.0, 0.0, 0.0, 0.0, 0.021251408383250237, 0.0, 0.07947628200054169, 0.0, 0.0, 0.1827886700630188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022280562669038773, 0.0, 0.0, 0.0, 0.0, 0.12048318982124329, 0.0, 0.03439417481422424, 0.17808890342712402, 0.0468798503279686, 0.030102640390396118, 0.0625356137752533, 0.06390024721622467, 0.0, 0.05231620743870735, 0.0, 0.04097406193614006, 0.040544986724853516, 0.0, 0.0, 0.0, 0.002922849729657173, 0.006490979343652725, 0.022413115948438644, 0.0, 0.08777956664562225, 0.0, 0.0, 0.14787718653678894, 0.034407131373882294, 0.05953465774655342, 0.0, 0.004971768707036972, 0.0, 0.0, 0.03896694257855415, 0.06627200543880463, 0.042677171528339386, 0.08695799112319946, 0.08620362728834152, 0.07567951828241348, 0.0886358693242073, 0.16243237257003784, 0.06760141253471375, 0.13317936658859253, 0.0, 0.0, 0.0, 0.006300399079918861, 0.0, 0.14069223403930664, 0.010742710903286934, 0.0, 0.0, 0.0, 0.10574249923229218, 0.0, 0.0, 0.0, 0.029449228197336197, 0.05125148594379425, 0.0007259547710418701, 0.08784649521112442, 0.09213638305664062, 0.0451744943857193, 0.07683254778385162, 0.0, 0.002177078276872635, 0.0, 0.0, 0.02540874294936657, 0.04761457443237305, 0.1175692230463028, 0.0, 0.12571977078914642, 0.0626932680606842, 0.08290689438581467, 0.09831444919109344, 0.023804057389497757, 0.01388099230825901, 0.030513711273670197, 0.09765107184648514, 0.0, 0.08097389340400696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07321454584598541, 0.045660194009542465, 0.013437889516353607, 0.0, 0.0, 0.0, 0.0, 0.0478924922645092, 0.0, 0.0, 0.06790286302566528, 0.0, 0.025777393952012062, 0.0, 0.06148412823677063, 0.0, 0.0, 0.02455981820821762, 0.06847386062145233, 0.10078435391187668, 0.08338386565446854, 0.0, 0.0, 0.0, 0.052317749708890915, 0.0, 0.0, 0.04634967818856239, 0.19864216446876526, 0.0019312240183353424, 0.0359635166823864, 0.0, 0.16408434510231018, 0.0, 0.0, 0.00898662768304348, 0.08771857619285583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040507715195417404, 0.0, 0.11521834135055542, 0.0, 0.07207836210727692, 0.07210316509008408, 0.0, 0.05518466979265213, 0.0666561871767044, 0.083528533577919]\n"
     ]
    }
   ],
   "source": [
    "# Example Arabic sentence\n",
    "sentence = \"انا احب\"\n",
    "\n",
    "# Tokenize and obtain contextual embeddings\n",
    "tokens = tokenizer(sentence, return_tensors='pt')\n",
    "outputs = model(**tokens)\n",
    "\n",
    "# The last layer hidden states often contain contextual embeddings\n",
    "contextual_embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Apply global average pooling (GAP) along the sequence dimension\n",
    "global_avg_pooled_embedding = torch.mean(contextual_embeddings, dim=1)\n",
    "\n",
    "# If necessary, apply linear transformation to get embeddings of size 100\n",
    "desired_embedding_size = 300\n",
    "linear_layer = torch.nn.Linear(global_avg_pooled_embedding.size(-1), desired_embedding_size)\n",
    "transformed_embedding = linear_layer(global_avg_pooled_embedding)\n",
    "\n",
    "# Apply activation function (e.g., ReLU)\n",
    "transformed_embedding = torch.relu(transformed_embedding)\n",
    "\n",
    "# For demonstration, let's print the transformed embedding\n",
    "print(\"Transformed Embedding:\")\n",
    "print(transformed_embedding[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

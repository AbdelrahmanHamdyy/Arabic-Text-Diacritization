{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ق', 'و', 'ل', 'ه'], ['ل', 'ع', 'د', 'م'], ['م', 'ا'], ['ت', 'ت', 'ع', 'ل', 'ق'], ['إ', 'ل', 'خ'], ['أ', 'ي'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['ق', 'و', 'ل', 'ه'], ['م', 'ا'], ['م', 'ر'], ['أ', 'ي'], ['ق', 'ب', 'ي', 'ل'], ['ق', 'و', 'ل'], ['ا', 'ل', 'م', 'ت', 'ن'], ['ل', 'غ', 'ت'], ['و', 'ل', 'و'], ['ا', 'ق', 'ت', 'ص', 'ر'], ['ع', 'ل', 'ى'], ['أ', 'و', 'ص', 'ي', 'ت'], ['ل', 'ه'], ['ب', 'ش', 'ا', 'ة'], ['أ', 'و'], ['أ', 'ع', 'ط', 'و', 'ه'], ['ش', 'ا', 'ة'], ['و', 'ل', 'ا'], ['غ', 'ن', 'م'], ['ل', 'ه'], ['ع', 'ن', 'د'], ['ا', 'ل', 'م', 'و', 'ت'], ['ه', 'ل'], ['ت', 'ب', 'ط', 'ل'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['أ', 'و'], ['ي', 'ش', 'ت', 'ر', 'ى'], ['ل', 'ه'], ['ش', 'ا', 'ة'], ['و', 'ي', 'ؤ', 'خ', 'ذ'], ['م', 'ن'], ['ق', 'و', 'ل', 'ه'], ['ا', 'ل', 'آ', 'ت', 'ي'], ['ك', 'م', 'ا'], ['ل', 'و'], ['ل', 'م'], ['ي', 'ق', 'ل'], ['م', 'ن'], ['م', 'ا', 'ل', 'ي'], ['و', 'ل', 'ا'], ['م', 'ن'], ['غ', 'ن', 'م', 'ي'], ['أ', 'ن', 'ه', 'ا'], ['ل', 'ا'], ['ت', 'ب', 'ط', 'ل'], ['و', 'ع', 'ب', 'ا', 'ر', 'ة'], ['ا', 'ل', 'ك', 'ن', 'ز'], ['و', 'ل', 'و'], ['ل', 'م'], ['ي', 'ق', 'ل'], ['م', 'ن'], ['م', 'ا', 'ل', 'ي'], ['و', 'ل', 'ا'], ['م', 'ن'], ['غ', 'ن', 'م', 'ي'], ['ل', 'م'], ['ي', 'ت', 'ع', 'ي', 'ن'], ['غ', 'ن', 'م', 'ه'], ['إ', 'ن'], ['ك', 'ا', 'ن', 'ت'], ['ا', 'ن', 'ت', 'ه', 'ت'], ['س', 'م'], ['ق', 'و', 'ل', 'ه'], ['ف', 'ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن', 'ه', 'ا'], ['إ', 'ل', 'خ'], ['ك', 'م', 'ا'], ['ل', 'و'], ['ك', 'ا', 'ن', 'ت'], ['م', 'و', 'ج', 'و', 'د', 'ة'], ['ع', 'ن', 'د'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['و', 'ا', 'ل', 'م', 'و', 'ت'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن'], ['غ', 'ي', 'ر'], ['غ', 'ن', 'م', 'ه'], ['ف', 'ي'], ['ا', 'ل', 'ص', 'و', 'ر', 'ت', 'ي', 'ن'], ['و', 'إ', 'ن'], ['ت', 'ر', 'ا', 'ض', 'ي', 'ا'], ['ل', 'أ', 'ن', 'ه'], ['ص', 'ل', 'ح'], ['ع', 'ل', 'ى'], ['م', 'ج', 'ه', 'و', 'ل'], ['م', 'غ', 'ن', 'ي'], ['و', 'ن', 'ه', 'ا', 'ي', 'ة'], ['ق', 'ا', 'ل'], [], [], ['ق', 'و', 'ل', 'ه'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن', 'ه', 'ا'], ['أ', 'ي'], ['ك', 'ا', 'م', 'ل', 'ة'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['ن', 'ص', 'ف', 'ي', 'ن'], ['م', 'ن'], ['ش', 'ا', 'ت', 'ي', 'ن'], ['ل', 'أ', 'ن', 'ه'], ['ل', 'ا'], ['ي', 'س', 'م', 'ى'], ['ش', 'ا', 'ة'], ['و', 'ق', 'و', 'ل', 'ه'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن']]\n",
      "[['ق', 'و', 'ل', 'ه'], ['ل', 'ع', 'د', 'م'], ['م', 'ا'], ['ت', 'ت', 'ع', 'ل', 'ق'], ['إ', 'ل', 'خ'], ['أ', 'ي'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['ق', 'و', 'ل', 'ه'], ['م', 'ا'], ['م', 'ر'], ['أ', 'ي'], ['ق', 'ب', 'ي', 'ل'], ['ق', 'و', 'ل'], ['ا', 'ل', 'م', 'ت', 'ن'], ['ل', 'غ', 'ت'], ['و', 'ل', 'و'], ['ا', 'ق', 'ت', 'ص', 'ر'], ['ع', 'ل', 'ى'], ['أ', 'و', 'ص', 'ي', 'ت'], ['ل', 'ه'], ['ب', 'ش', 'ا', 'ة'], ['أ', 'و'], ['أ', 'ع', 'ط', 'و', 'ه'], ['ش', 'ا', 'ة'], ['و', 'ل', 'ا'], ['غ', 'ن', 'م'], ['ل', 'ه'], ['ع', 'ن', 'د'], ['ا', 'ل', 'م', 'و', 'ت'], ['ه', 'ل'], ['ت', 'ب', 'ط', 'ل'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['أ', 'و'], ['ي', 'ش', 'ت', 'ر', 'ى'], ['ل', 'ه'], ['ش', 'ا', 'ة'], ['و', 'ي', 'ؤ', 'خ', 'ذ'], ['م', 'ن'], ['ق', 'و', 'ل', 'ه'], ['ا', 'ل', 'آ', 'ت', 'ي'], ['ك', 'م', 'ا'], ['ل', 'و'], ['ل', 'م'], ['ي', 'ق', 'ل'], ['م', 'ن'], ['م', 'ا', 'ل', 'ي'], ['و', 'ل', 'ا'], ['م', 'ن'], ['غ', 'ن', 'م', 'ي'], ['أ', 'ن', 'ه', 'ا'], ['ل', 'ا'], ['ت', 'ب', 'ط', 'ل'], ['و', 'ع', 'ب', 'ا', 'ر', 'ة'], ['ا', 'ل', 'ك', 'ن', 'ز'], ['و', 'ل', 'و'], ['ل', 'م'], ['ي', 'ق', 'ل'], ['م', 'ن'], ['م', 'ا', 'ل', 'ي'], ['و', 'ل', 'ا'], ['م', 'ن'], ['غ', 'ن', 'م', 'ي'], ['ل', 'م'], ['ي', 'ت', 'ع', 'ي', 'ن'], ['غ', 'ن', 'م', 'ه'], ['إ', 'ن'], ['ك', 'ا', 'ن', 'ت'], ['ا', 'ن', 'ت', 'ه', 'ت'], ['س', 'م'], ['ق', 'و', 'ل', 'ه'], ['ف', 'ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن', 'ه', 'ا'], ['إ', 'ل', 'خ'], ['ك', 'م', 'ا'], ['ل', 'و'], ['ك', 'ا', 'ن', 'ت'], ['م', 'و', 'ج', 'و', 'د', 'ة'], ['ع', 'ن', 'د'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['و', 'ا', 'ل', 'م', 'و', 'ت'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن'], ['غ', 'ي', 'ر'], ['غ', 'ن', 'م', 'ه'], ['ف', 'ي'], ['ا', 'ل', 'ص', 'و', 'ر', 'ت', 'ي', 'ن'], ['و', 'إ', 'ن'], ['ت', 'ر', 'ا', 'ض', 'ي', 'ا'], ['ل', 'أ', 'ن', 'ه'], ['ص', 'ل', 'ح'], ['ع', 'ل', 'ى'], ['م', 'ج', 'ه', 'و', 'ل'], ['م', 'غ', 'ن', 'ي'], ['و', 'ن', 'ه', 'ا', 'ي', 'ة'], ['ق', 'ا', 'ل'], [], [], ['ق', 'و', 'ل', 'ه'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن', 'ه', 'ا'], ['أ', 'ي'], ['ك', 'ا', 'م', 'ل', 'ة'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['ن', 'ص', 'ف', 'ي', 'ن'], ['م', 'ن'], ['ش', 'ا', 'ت', 'ي', 'ن'], ['ل', 'أ', 'ن', 'ه'], ['ل', 'ا'], ['ي', 'س', 'م', 'ى'], ['ش', 'ا', 'ة'], ['و', 'ق', 'و', 'ل', 'ه'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن']]\n",
      "['قَوْلُهُ', 'لِعَدَمِ', 'مَا', 'تَتَعَلَّقُ']\n",
      "_________________\n",
      "['قَوْلُهُ', 'لِعَدَمِ', 'مَا', 'تَتَعَلَّقُ', 'إلَخْ']\n",
      "_________________\n",
      "['قَوْلُهُ', 'لِعَدَمِ', 'مَا', 'تَتَعَلَّقُ', 'إلَخْ', 'أَيْ']\n",
      "_________________\n",
      "['لِعَدَمِ', 'مَا', 'تَتَعَلَّقُ', 'إلَخْ', 'أَيْ', 'الْوَصِيَّةُ']\n",
      "_________________\n",
      "['مَا', 'تَتَعَلَّقُ', 'إلَخْ', 'أَيْ', 'الْوَصِيَّةُ', 'قَوْلُهُ']\n",
      "_________________\n",
      "['تَتَعَلَّقُ', 'إلَخْ', 'أَيْ', 'الْوَصِيَّةُ', 'قَوْلُهُ', 'مَا']\n",
      "_________________\n",
      "['إلَخْ', 'أَيْ', 'الْوَصِيَّةُ', 'قَوْلُهُ', 'مَا', 'مَرَّ']\n",
      "_________________\n",
      "['أَيْ', 'الْوَصِيَّةُ', 'قَوْلُهُ', 'مَا', 'مَرَّ', 'أَيْ']\n",
      "_________________\n",
      "['الْوَصِيَّةُ', 'قَوْلُهُ', 'مَا', 'مَرَّ', 'أَيْ', 'قُبَيْلَ']\n",
      "_________________\n",
      "['قَوْلُهُ', 'مَا', 'مَرَّ', 'أَيْ', 'قُبَيْلَ', 'قَوْلِ']\n",
      "_________________\n",
      "['مَا', 'مَرَّ', 'أَيْ', 'قُبَيْلَ', 'قَوْلِ', 'الْمَتْنِ']\n",
      "_________________\n",
      "['مَرَّ', 'أَيْ', 'قُبَيْلَ', 'قَوْلِ', 'الْمَتْنِ', 'لَغَتْ']\n",
      "_________________\n",
      "['أَيْ', 'قُبَيْلَ', 'قَوْلِ', 'الْمَتْنِ', 'لَغَتْ', 'وَلَوْ']\n",
      "_________________\n",
      "['قُبَيْلَ', 'قَوْلِ', 'الْمَتْنِ', 'لَغَتْ', 'وَلَوْ', 'اقْتَصَرَ']\n",
      "_________________\n",
      "['قَوْلِ', 'الْمَتْنِ', 'لَغَتْ', 'وَلَوْ', 'اقْتَصَرَ', 'عَلَى']\n",
      "_________________\n",
      "['الْمَتْنِ', 'لَغَتْ', 'وَلَوْ', 'اقْتَصَرَ', 'عَلَى', 'أَوْصَيْت']\n",
      "_________________\n",
      "['لَغَتْ', 'وَلَوْ', 'اقْتَصَرَ', 'عَلَى', 'أَوْصَيْت', 'لَهُ']\n",
      "_________________\n",
      "['وَلَوْ', 'اقْتَصَرَ', 'عَلَى', 'أَوْصَيْت', 'لَهُ', 'بِشَاةٍ']\n",
      "_________________\n",
      "['اقْتَصَرَ', 'عَلَى', 'أَوْصَيْت', 'لَهُ', 'بِشَاةٍ', 'أَوْ']\n",
      "_________________\n",
      "['عَلَى', 'أَوْصَيْت', 'لَهُ', 'بِشَاةٍ', 'أَوْ', 'أَعْطُوهُ']\n",
      "_________________\n",
      "['أَوْصَيْت', 'لَهُ', 'بِشَاةٍ', 'أَوْ', 'أَعْطُوهُ', 'شَاةً']\n",
      "_________________\n",
      "['لَهُ', 'بِشَاةٍ', 'أَوْ', 'أَعْطُوهُ', 'شَاةً', 'وَلَا']\n",
      "_________________\n",
      "['بِشَاةٍ', 'أَوْ', 'أَعْطُوهُ', 'شَاةً', 'وَلَا', 'غَنَمَ']\n",
      "_________________\n",
      "['أَوْ', 'أَعْطُوهُ', 'شَاةً', 'وَلَا', 'غَنَمَ', 'لَهُ']\n",
      "_________________\n",
      "['أَعْطُوهُ', 'شَاةً', 'وَلَا', 'غَنَمَ', 'لَهُ', 'عِنْدَ']\n",
      "_________________\n",
      "['شَاةً', 'وَلَا', 'غَنَمَ', 'لَهُ', 'عِنْدَ', 'الْمَوْتِ']\n",
      "_________________\n",
      "['وَلَا', 'غَنَمَ', 'لَهُ', 'عِنْدَ', 'الْمَوْتِ', 'هَلْ']\n",
      "_________________\n",
      "['غَنَمَ', 'لَهُ', 'عِنْدَ', 'الْمَوْتِ', 'هَلْ', 'تَبْطُلُ']\n",
      "_________________\n",
      "['لَهُ', 'عِنْدَ', 'الْمَوْتِ', 'هَلْ', 'تَبْطُلُ', 'الْوَصِيَّةُ']\n",
      "_________________\n",
      "['عِنْدَ', 'الْمَوْتِ', 'هَلْ', 'تَبْطُلُ', 'الْوَصِيَّةُ', 'أَوْ']\n",
      "_________________\n",
      "['الْمَوْتِ', 'هَلْ', 'تَبْطُلُ', 'الْوَصِيَّةُ', 'أَوْ', 'يُشْتَرَى']\n",
      "_________________\n",
      "['هَلْ', 'تَبْطُلُ', 'الْوَصِيَّةُ', 'أَوْ', 'يُشْتَرَى', 'لَهُ']\n",
      "_________________\n",
      "['تَبْطُلُ', 'الْوَصِيَّةُ', 'أَوْ', 'يُشْتَرَى', 'لَهُ', 'شَاةٌ']\n",
      "_________________\n",
      "['الْوَصِيَّةُ', 'أَوْ', 'يُشْتَرَى', 'لَهُ', 'شَاةٌ', 'وَيُؤْخَذُ']\n",
      "_________________\n",
      "['أَوْ', 'يُشْتَرَى', 'لَهُ', 'شَاةٌ', 'وَيُؤْخَذُ', 'مِنْ']\n",
      "_________________\n",
      "['يُشْتَرَى', 'لَهُ', 'شَاةٌ', 'وَيُؤْخَذُ', 'مِنْ', 'قَوْلِهِ']\n",
      "_________________\n",
      "['لَهُ', 'شَاةٌ', 'وَيُؤْخَذُ', 'مِنْ', 'قَوْلِهِ', 'الْآتِي']\n",
      "_________________\n",
      "['شَاةٌ', 'وَيُؤْخَذُ', 'مِنْ', 'قَوْلِهِ', 'الْآتِي', 'كَمَا']\n",
      "_________________\n",
      "['وَيُؤْخَذُ', 'مِنْ', 'قَوْلِهِ', 'الْآتِي', 'كَمَا', 'لَوْ']\n",
      "_________________\n",
      "['مِنْ', 'قَوْلِهِ', 'الْآتِي', 'كَمَا', 'لَوْ', 'لَمْ']\n",
      "_________________\n",
      "['قَوْلِهِ', 'الْآتِي', 'كَمَا', 'لَوْ', 'لَمْ', 'يَقُلْ']\n",
      "_________________\n",
      "['الْآتِي', 'كَمَا', 'لَوْ', 'لَمْ', 'يَقُلْ', 'مِنْ']\n",
      "_________________\n",
      "['كَمَا', 'لَوْ', 'لَمْ', 'يَقُلْ', 'مِنْ', 'مَالِي']\n",
      "_________________\n",
      "['لَوْ', 'لَمْ', 'يَقُلْ', 'مِنْ', 'مَالِي', 'وَلَا']\n",
      "_________________\n",
      "['لَمْ', 'يَقُلْ', 'مِنْ', 'مَالِي', 'وَلَا', 'مِنْ']\n",
      "_________________\n",
      "['يَقُلْ', 'مِنْ', 'مَالِي', 'وَلَا', 'مِنْ', 'غَنَمِي']\n",
      "_________________\n",
      "['مِنْ', 'مَالِي', 'وَلَا', 'مِنْ', 'غَنَمِي', 'أَنَّهَا']\n",
      "_________________\n",
      "['مَالِي', 'وَلَا', 'مِنْ', 'غَنَمِي', 'أَنَّهَا', 'لَا']\n",
      "_________________\n",
      "['وَلَا', 'مِنْ', 'غَنَمِي', 'أَنَّهَا', 'لَا', 'تَبْطُلُ']\n",
      "_________________\n",
      "['مِنْ', 'غَنَمِي', 'أَنَّهَا', 'لَا', 'تَبْطُلُ', 'وَعِبَارَةُ']\n",
      "_________________\n",
      "['غَنَمِي', 'أَنَّهَا', 'لَا', 'تَبْطُلُ', 'وَعِبَارَةُ', 'الْكَنْزِ']\n",
      "_________________\n",
      "['أَنَّهَا', 'لَا', 'تَبْطُلُ', 'وَعِبَارَةُ', 'الْكَنْزِ', 'وَلَوْ']\n",
      "_________________\n",
      "['لَا', 'تَبْطُلُ', 'وَعِبَارَةُ', 'الْكَنْزِ', 'وَلَوْ', 'لَمْ']\n",
      "_________________\n",
      "['تَبْطُلُ', 'وَعِبَارَةُ', 'الْكَنْزِ', 'وَلَوْ', 'لَمْ', 'يَقُلْ']\n",
      "_________________\n",
      "['وَعِبَارَةُ', 'الْكَنْزِ', 'وَلَوْ', 'لَمْ', 'يَقُلْ', 'مِنْ']\n",
      "_________________\n",
      "['الْكَنْزِ', 'وَلَوْ', 'لَمْ', 'يَقُلْ', 'مِنْ', 'مَالِي']\n",
      "_________________\n",
      "['وَلَوْ', 'لَمْ', 'يَقُلْ', 'مِنْ', 'مَالِي', 'وَلَا']\n",
      "_________________\n",
      "['لَمْ', 'يَقُلْ', 'مِنْ', 'مَالِي', 'وَلَا', 'مِنْ']\n",
      "_________________\n",
      "['يَقُلْ', 'مِنْ', 'مَالِي', 'وَلَا', 'مِنْ', 'غَنَمِي']\n",
      "_________________\n",
      "['مِنْ', 'مَالِي', 'وَلَا', 'مِنْ', 'غَنَمِي', 'لَمْ']\n",
      "_________________\n",
      "['مَالِي', 'وَلَا', 'مِنْ', 'غَنَمِي', 'لَمْ', 'يَتَعَيَّنْ']\n",
      "_________________\n",
      "['وَلَا', 'مِنْ', 'غَنَمِي', 'لَمْ', 'يَتَعَيَّنْ', 'غَنَمُهُ']\n",
      "_________________\n",
      "['مِنْ', 'غَنَمِي', 'لَمْ', 'يَتَعَيَّنْ', 'غَنَمُهُ', 'إنْ']\n",
      "_________________\n",
      "['غَنَمِي', 'لَمْ', 'يَتَعَيَّنْ', 'غَنَمُهُ', 'إنْ', 'كَانَتْ']\n",
      "_________________\n",
      "['لَمْ', 'يَتَعَيَّنْ', 'غَنَمُهُ', 'إنْ', 'كَانَتْ', 'انْتَهَتْ']\n",
      "_________________\n",
      "['يَتَعَيَّنْ', 'غَنَمُهُ', 'إنْ', 'كَانَتْ', 'انْتَهَتْ', 'سم']\n",
      "_________________\n",
      "['غَنَمُهُ', 'إنْ', 'كَانَتْ', 'انْتَهَتْ', 'سم', 'قَوْلُهُ']\n",
      "_________________\n",
      "['إنْ', 'كَانَتْ', 'انْتَهَتْ', 'سم', 'قَوْلُهُ', 'فَيُعْطَى']\n",
      "_________________\n",
      "['كَانَتْ', 'انْتَهَتْ', 'سم', 'قَوْلُهُ', 'فَيُعْطَى', 'وَاحِدَةً']\n",
      "_________________\n",
      "['انْتَهَتْ', 'سم', 'قَوْلُهُ', 'فَيُعْطَى', 'وَاحِدَةً', 'مِنْهَا']\n",
      "_________________\n",
      "['سم', 'قَوْلُهُ', 'فَيُعْطَى', 'وَاحِدَةً', 'مِنْهَا', 'إلَخْ']\n",
      "_________________\n",
      "['قَوْلُهُ', 'فَيُعْطَى', 'وَاحِدَةً', 'مِنْهَا', 'إلَخْ', 'كَمَا']\n",
      "_________________\n",
      "['فَيُعْطَى', 'وَاحِدَةً', 'مِنْهَا', 'إلَخْ', 'كَمَا', 'لَوْ']\n",
      "_________________\n",
      "['وَاحِدَةً', 'مِنْهَا', 'إلَخْ', 'كَمَا', 'لَوْ', 'كَانَتْ']\n",
      "_________________\n",
      "['مِنْهَا', 'إلَخْ', 'كَمَا', 'لَوْ', 'كَانَتْ', 'مَوْجُودَةً']\n",
      "_________________\n",
      "['إلَخْ', 'كَمَا', 'لَوْ', 'كَانَتْ', 'مَوْجُودَةً', 'عِنْدَ']\n",
      "_________________\n",
      "['كَمَا', 'لَوْ', 'كَانَتْ', 'مَوْجُودَةً', 'عِنْدَ', 'الْوَصِيَّةِ']\n",
      "_________________\n",
      "['لَوْ', 'كَانَتْ', 'مَوْجُودَةً', 'عِنْدَ', 'الْوَصِيَّةِ', 'وَالْمَوْتِ']\n",
      "_________________\n",
      "['كَانَتْ', 'مَوْجُودَةً', 'عِنْدَ', 'الْوَصِيَّةِ', 'وَالْمَوْتِ', 'وَلَا']\n",
      "_________________\n",
      "['مَوْجُودَةً', 'عِنْدَ', 'الْوَصِيَّةِ', 'وَالْمَوْتِ', 'وَلَا', 'يَجُوزُ']\n",
      "_________________\n",
      "['عِنْدَ', 'الْوَصِيَّةِ', 'وَالْمَوْتِ', 'وَلَا', 'يَجُوزُ', 'أَنْ']\n",
      "_________________\n",
      "['الْوَصِيَّةِ', 'وَالْمَوْتِ', 'وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى']\n",
      "_________________\n",
      "['وَالْمَوْتِ', 'وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى', 'وَاحِدَةً']\n",
      "_________________\n",
      "['وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى', 'وَاحِدَةً', 'مِنْ']\n",
      "_________________\n",
      "['يَجُوزُ', 'أَنْ', 'يُعْطَى', 'وَاحِدَةً', 'مِنْ', 'غَيْرِ']\n",
      "_________________\n",
      "['أَنْ', 'يُعْطَى', 'وَاحِدَةً', 'مِنْ', 'غَيْرِ', 'غَنَمِهِ']\n",
      "_________________\n",
      "['يُعْطَى', 'وَاحِدَةً', 'مِنْ', 'غَيْرِ', 'غَنَمِهِ', 'فِي']\n",
      "_________________\n",
      "['وَاحِدَةً', 'مِنْ', 'غَيْرِ', 'غَنَمِهِ', 'فِي', 'الصُّورَتَيْنِ']\n",
      "_________________\n",
      "['مِنْ', 'غَيْرِ', 'غَنَمِهِ', 'فِي', 'الصُّورَتَيْنِ', 'وَإِنْ']\n",
      "_________________\n",
      "['غَيْرِ', 'غَنَمِهِ', 'فِي', 'الصُّورَتَيْنِ', 'وَإِنْ', 'تَرَاضَيَا']\n",
      "_________________\n",
      "['غَنَمِهِ', 'فِي', 'الصُّورَتَيْنِ', 'وَإِنْ', 'تَرَاضَيَا', 'لِأَنَّهُ']\n",
      "_________________\n",
      "['فِي', 'الصُّورَتَيْنِ', 'وَإِنْ', 'تَرَاضَيَا', 'لِأَنَّهُ', 'صُلْحٌ']\n",
      "_________________\n",
      "['الصُّورَتَيْنِ', 'وَإِنْ', 'تَرَاضَيَا', 'لِأَنَّهُ', 'صُلْحٌ', 'عَلَى']\n",
      "_________________\n",
      "['وَإِنْ', 'تَرَاضَيَا', 'لِأَنَّهُ', 'صُلْحٌ', 'عَلَى', 'مَجْهُولٍ']\n",
      "_________________\n",
      "['تَرَاضَيَا', 'لِأَنَّهُ', 'صُلْحٌ', 'عَلَى', 'مَجْهُولٍ', 'مُغْنِي']\n",
      "_________________\n",
      "['لِأَنَّهُ', 'صُلْحٌ', 'عَلَى', 'مَجْهُولٍ', 'مُغْنِي', 'وَنِهَايَةٌ']\n",
      "_________________\n",
      "['صُلْحٌ', 'عَلَى', 'مَجْهُولٍ', 'مُغْنِي', 'وَنِهَايَةٌ', 'قَالَ']\n",
      "_________________\n",
      "['عَلَى', 'مَجْهُولٍ', 'مُغْنِي', 'وَنِهَايَةٌ', 'قَالَ', 'ع']\n",
      "_________________\n",
      "['مَجْهُولٍ', 'مُغْنِي', 'وَنِهَايَةٌ', 'قَالَ', 'ع', 'ش']\n",
      "_________________\n",
      "['مُغْنِي', 'وَنِهَايَةٌ', 'قَالَ', 'ع', 'ش', 'قَوْلُهُ']\n",
      "_________________\n",
      "['وَنِهَايَةٌ', 'قَالَ', 'ع', 'ش', 'قَوْلُهُ', 'وَاحِدَةً']\n",
      "_________________\n",
      "['قَالَ', 'ع', 'ش', 'قَوْلُهُ', 'وَاحِدَةً', 'مِنْهَا']\n",
      "_________________\n",
      "['ع', 'ش', 'قَوْلُهُ', 'وَاحِدَةً', 'مِنْهَا', 'أَيْ']\n",
      "_________________\n",
      "['ش', 'قَوْلُهُ', 'وَاحِدَةً', 'مِنْهَا', 'أَيْ', 'كَامِلَةً']\n",
      "_________________\n",
      "['قَوْلُهُ', 'وَاحِدَةً', 'مِنْهَا', 'أَيْ', 'كَامِلَةً', 'وَلَا']\n",
      "_________________\n",
      "['وَاحِدَةً', 'مِنْهَا', 'أَيْ', 'كَامِلَةً', 'وَلَا', 'يَجُوزُ']\n",
      "_________________\n",
      "['مِنْهَا', 'أَيْ', 'كَامِلَةً', 'وَلَا', 'يَجُوزُ', 'أَنْ']\n",
      "_________________\n",
      "['أَيْ', 'كَامِلَةً', 'وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى']\n",
      "_________________\n",
      "['كَامِلَةً', 'وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى', 'نِصْفَيْنِ']\n",
      "_________________\n",
      "['وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى', 'نِصْفَيْنِ', 'مِنْ']\n",
      "_________________\n",
      "['يَجُوزُ', 'أَنْ', 'يُعْطَى', 'نِصْفَيْنِ', 'مِنْ', 'شَاتَيْنِ']\n",
      "_________________\n",
      "['أَنْ', 'يُعْطَى', 'نِصْفَيْنِ', 'مِنْ', 'شَاتَيْنِ', 'لِأَنَّهُ']\n",
      "_________________\n",
      "['يُعْطَى', 'نِصْفَيْنِ', 'مِنْ', 'شَاتَيْنِ', 'لِأَنَّهُ', 'لَا']\n",
      "_________________\n",
      "['نِصْفَيْنِ', 'مِنْ', 'شَاتَيْنِ', 'لِأَنَّهُ', 'لَا', 'يُسَمَّى']\n",
      "_________________\n",
      "['مِنْ', 'شَاتَيْنِ', 'لِأَنَّهُ', 'لَا', 'يُسَمَّى', 'شَاةً']\n",
      "_________________\n",
      "['شَاتَيْنِ', 'لِأَنَّهُ', 'لَا', 'يُسَمَّى', 'شَاةً', 'وَقَوْلُهُ']\n",
      "_________________\n",
      "['لِأَنَّهُ', 'لَا', 'يُسَمَّى', 'شَاةً', 'وَقَوْلُهُ', 'وَلَا']\n",
      "_________________\n",
      "['لَا', 'يُسَمَّى', 'شَاةً', 'وَقَوْلُهُ', 'وَلَا', 'يَجُوزُ']\n",
      "_________________\n",
      "['يُسَمَّى', 'شَاةً', 'وَقَوْلُهُ', 'وَلَا', 'يَجُوزُ', 'أَنْ']\n",
      "_________________\n",
      "['شَاةً', 'وَقَوْلُهُ', 'وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى']\n",
      "_________________\n",
      "['وَقَوْلُهُ', 'وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى', 'وَاحِدَةً']\n",
      "_________________\n",
      "['وَلَا', 'يَجُوزُ', 'أَنْ', 'يُعْطَى', 'وَاحِدَةً', 'مِنْ']\n",
      "_________________\n",
      "['يَجُوزُ', 'أَنْ', 'يُعْطَى', 'وَاحِدَةً', 'مِنْ']\n",
      "_________________\n",
      "['أَنْ', 'يُعْطَى', 'وَاحِدَةً', 'مِنْ']\n",
      "_________________\n",
      "['يُعْطَى', 'وَاحِدَةً', 'مِنْ']\n",
      "_________________\n",
      "4\n",
      "['ق', 'و', 'ل', 'ه']\n",
      "-----------------\n",
      "4\n",
      "['ل', 'ع', 'د', 'م']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ا']\n",
      "-----------------\n",
      "5\n",
      "['ت', 'ت', 'ع', 'ل', 'ق']\n",
      "-----------------\n",
      "3\n",
      "['إ', 'ل', 'خ']\n",
      "-----------------\n",
      "2\n",
      "['أ', 'ي']\n",
      "-----------------\n",
      "6\n",
      "['ا', 'ل', 'و', 'ص', 'ي', 'ة']\n",
      "-----------------\n",
      "4\n",
      "['ق', 'و', 'ل', 'ه']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ا']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ر']\n",
      "-----------------\n",
      "2\n",
      "['أ', 'ي']\n",
      "-----------------\n",
      "4\n",
      "['ق', 'ب', 'ي', 'ل']\n",
      "-----------------\n",
      "3\n",
      "['ق', 'و', 'ل']\n",
      "-----------------\n",
      "5\n",
      "['ا', 'ل', 'م', 'ت', 'ن']\n",
      "-----------------\n",
      "3\n",
      "['ل', 'غ', 'ت']\n",
      "-----------------\n",
      "3\n",
      "['و', 'ل', 'و']\n",
      "-----------------\n",
      "5\n",
      "['ا', 'ق', 'ت', 'ص', 'ر']\n",
      "-----------------\n",
      "3\n",
      "['ع', 'ل', 'ى']\n",
      "-----------------\n",
      "5\n",
      "['أ', 'و', 'ص', 'ي', 'ت']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'ه']\n",
      "-----------------\n",
      "4\n",
      "['ب', 'ش', 'ا', 'ة']\n",
      "-----------------\n",
      "2\n",
      "['أ', 'و']\n",
      "-----------------\n",
      "5\n",
      "['أ', 'ع', 'ط', 'و', 'ه']\n",
      "-----------------\n",
      "3\n",
      "['ش', 'ا', 'ة']\n",
      "-----------------\n",
      "3\n",
      "['و', 'ل', 'ا']\n",
      "-----------------\n",
      "3\n",
      "['غ', 'ن', 'م']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'ه']\n",
      "-----------------\n",
      "3\n",
      "['ع', 'ن', 'د']\n",
      "-----------------\n",
      "5\n",
      "['ا', 'ل', 'م', 'و', 'ت']\n",
      "-----------------\n",
      "2\n",
      "['ه', 'ل']\n",
      "-----------------\n",
      "4\n",
      "['ت', 'ب', 'ط', 'ل']\n",
      "-----------------\n",
      "6\n",
      "['ا', 'ل', 'و', 'ص', 'ي', 'ة']\n",
      "-----------------\n",
      "2\n",
      "['أ', 'و']\n",
      "-----------------\n",
      "5\n",
      "['ي', 'ش', 'ت', 'ر', 'ى']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'ه']\n",
      "-----------------\n",
      "3\n",
      "['ش', 'ا', 'ة']\n",
      "-----------------\n",
      "5\n",
      "['و', 'ي', 'ؤ', 'خ', 'ذ']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['ق', 'و', 'ل', 'ه']\n",
      "-----------------\n",
      "5\n",
      "['ا', 'ل', 'آ', 'ت', 'ي']\n",
      "-----------------\n",
      "3\n",
      "['ك', 'م', 'ا']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'و']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'م']\n",
      "-----------------\n",
      "3\n",
      "['ي', 'ق', 'ل']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['م', 'ا', 'ل', 'ي']\n",
      "-----------------\n",
      "3\n",
      "['و', 'ل', 'ا']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['غ', 'ن', 'م', 'ي']\n",
      "-----------------\n",
      "4\n",
      "['أ', 'ن', 'ه', 'ا']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'ا']\n",
      "-----------------\n",
      "4\n",
      "['ت', 'ب', 'ط', 'ل']\n",
      "-----------------\n",
      "6\n",
      "['و', 'ع', 'ب', 'ا', 'ر', 'ة']\n",
      "-----------------\n",
      "5\n",
      "['ا', 'ل', 'ك', 'ن', 'ز']\n",
      "-----------------\n",
      "3\n",
      "['و', 'ل', 'و']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'م']\n",
      "-----------------\n",
      "3\n",
      "['ي', 'ق', 'ل']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['م', 'ا', 'ل', 'ي']\n",
      "-----------------\n",
      "3\n",
      "['و', 'ل', 'ا']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['غ', 'ن', 'م', 'ي']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'م']\n",
      "-----------------\n",
      "5\n",
      "['ي', 'ت', 'ع', 'ي', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['غ', 'ن', 'م', 'ه']\n",
      "-----------------\n",
      "2\n",
      "['إ', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['ك', 'ا', 'ن', 'ت']\n",
      "-----------------\n",
      "5\n",
      "['ا', 'ن', 'ت', 'ه', 'ت']\n",
      "-----------------\n",
      "2\n",
      "['س', 'م']\n",
      "-----------------\n",
      "4\n",
      "['ق', 'و', 'ل', 'ه']\n",
      "-----------------\n",
      "5\n",
      "['ف', 'ي', 'ع', 'ط', 'ى']\n",
      "-----------------\n",
      "5\n",
      "['و', 'ا', 'ح', 'د', 'ة']\n",
      "-----------------\n",
      "4\n",
      "['م', 'ن', 'ه', 'ا']\n",
      "-----------------\n",
      "3\n",
      "['إ', 'ل', 'خ']\n",
      "-----------------\n",
      "3\n",
      "['ك', 'م', 'ا']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'و']\n",
      "-----------------\n",
      "4\n",
      "['ك', 'ا', 'ن', 'ت']\n",
      "-----------------\n",
      "6\n",
      "['م', 'و', 'ج', 'و', 'د', 'ة']\n",
      "-----------------\n",
      "3\n",
      "['ع', 'ن', 'د']\n",
      "-----------------\n",
      "6\n",
      "['ا', 'ل', 'و', 'ص', 'ي', 'ة']\n",
      "-----------------\n",
      "6\n",
      "['و', 'ا', 'ل', 'م', 'و', 'ت']\n",
      "-----------------\n",
      "3\n",
      "['و', 'ل', 'ا']\n",
      "-----------------\n",
      "4\n",
      "['ي', 'ج', 'و', 'ز']\n",
      "-----------------\n",
      "2\n",
      "['أ', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['ي', 'ع', 'ط', 'ى']\n",
      "-----------------\n",
      "5\n",
      "['و', 'ا', 'ح', 'د', 'ة']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ن']\n",
      "-----------------\n",
      "3\n",
      "['غ', 'ي', 'ر']\n",
      "-----------------\n",
      "4\n",
      "['غ', 'ن', 'م', 'ه']\n",
      "-----------------\n",
      "2\n",
      "['ف', 'ي']\n",
      "-----------------\n",
      "8\n",
      "['ا', 'ل', 'ص', 'و', 'ر', 'ت', 'ي', 'ن']\n",
      "-----------------\n",
      "3\n",
      "['و', 'إ', 'ن']\n",
      "-----------------\n",
      "6\n",
      "['ت', 'ر', 'ا', 'ض', 'ي', 'ا']\n",
      "-----------------\n",
      "4\n",
      "['ل', 'أ', 'ن', 'ه']\n",
      "-----------------\n",
      "3\n",
      "['ص', 'ل', 'ح']\n",
      "-----------------\n",
      "3\n",
      "['ع', 'ل', 'ى']\n",
      "-----------------\n",
      "5\n",
      "['م', 'ج', 'ه', 'و', 'ل']\n",
      "-----------------\n",
      "4\n",
      "['م', 'غ', 'ن', 'ي']\n",
      "-----------------\n",
      "6\n",
      "['و', 'ن', 'ه', 'ا', 'ي', 'ة']\n",
      "-----------------\n",
      "3\n",
      "['ق', 'ا', 'ل']\n",
      "-----------------\n",
      "0\n",
      "[]\n",
      "-----------------\n",
      "0\n",
      "[]\n",
      "-----------------\n",
      "4\n",
      "['ق', 'و', 'ل', 'ه']\n",
      "-----------------\n",
      "5\n",
      "['و', 'ا', 'ح', 'د', 'ة']\n",
      "-----------------\n",
      "4\n",
      "['م', 'ن', 'ه', 'ا']\n",
      "-----------------\n",
      "2\n",
      "['أ', 'ي']\n",
      "-----------------\n",
      "5\n",
      "['ك', 'ا', 'م', 'ل', 'ة']\n",
      "-----------------\n",
      "3\n",
      "['و', 'ل', 'ا']\n",
      "-----------------\n",
      "4\n",
      "['ي', 'ج', 'و', 'ز']\n",
      "-----------------\n",
      "2\n",
      "['أ', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['ي', 'ع', 'ط', 'ى']\n",
      "-----------------\n",
      "5\n",
      "['ن', 'ص', 'ف', 'ي', 'ن']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ن']\n",
      "-----------------\n",
      "5\n",
      "['ش', 'ا', 'ت', 'ي', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['ل', 'أ', 'ن', 'ه']\n",
      "-----------------\n",
      "2\n",
      "['ل', 'ا']\n",
      "-----------------\n",
      "4\n",
      "['ي', 'س', 'م', 'ى']\n",
      "-----------------\n",
      "3\n",
      "['ش', 'ا', 'ة']\n",
      "-----------------\n",
      "5\n",
      "['و', 'ق', 'و', 'ل', 'ه']\n",
      "-----------------\n",
      "3\n",
      "['و', 'ل', 'ا']\n",
      "-----------------\n",
      "4\n",
      "['ي', 'ج', 'و', 'ز']\n",
      "-----------------\n",
      "2\n",
      "['أ', 'ن']\n",
      "-----------------\n",
      "4\n",
      "['ي', 'ع', 'ط', 'ى']\n",
      "-----------------\n",
      "5\n",
      "['و', 'ا', 'ح', 'د', 'ة']\n",
      "-----------------\n",
      "2\n",
      "['م', 'ن']\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from TorchCRF import CRF\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "import time\n",
    "\n",
    "%run updatePreprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "VOCAB_SIZE = len(basic_arabic_letters) + 1\n",
    "LABELS_SIZE = len(DIACRITICS)\n",
    "WINDOW_SIZE_BEFORE = 7\n",
    "WINDOW_SIZE_AFTER = 3\n",
    "PAD=50\n",
    "\n",
    "TRAIN_PATH = \"../dataset/train.txt\"\n",
    "VAL_PATH = \"../dataset/val.txt\"\n",
    "LSTM_PATH=\"./models/lstm.pth\"\n",
    "RNN_PATH=\"./models/rnn.pth\"\n",
    "CNN_PATH = \"./models/cnn.pth\"\n",
    "CRF_Val_PATH=\"./models/crf_val.pth\"\n",
    "CRF_PATH=\"./models/crf.pth\"\n",
    "CNN_val_PATH=\"./models/cnn_val.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"There are {torch.cuda.device_count()} GPU(s) available.\")\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        \"\"\"\n",
    "        The constructor of our RNN model\n",
    "        Inputs:\n",
    "        - vacab_size: the number of unique characters\n",
    "        - embedding_dim: the embedding dimension\n",
    "        - n_classes: the number of final classes (diacritics)\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # (1) Create an embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # (3) Create a linear layer with number of neorons = n_classes\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        \"\"\"\n",
    "        This function does the forward pass of our model\n",
    "        Inputs:\n",
    "        - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "        Returns:\n",
    "        - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        final_output = None\n",
    "        \n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        # final_output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Max pooling layers\n",
    "        # self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"1\", x.shape)\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(\"2\", x.shape)\n",
    "        # Convolutional layers with ReLU activation and max pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # x = self.pool(x)\n",
    "        print(\"3\", x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # x = self.pool(x)\n",
    "        print(\"4\", x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # x = self.pool(x)\n",
    "        print(\"5\", x.shape)\n",
    "\n",
    "        # Fully connected layers with ReLU activation\n",
    "        x = x.view(-1, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(\"6\", x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print(\"7\", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        super(RNN_CNN, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # CNN layer\n",
    "        self.conv1d = nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3, padding=1)  # Adjust parameters as needed\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(64, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeddings = self.embedding(sentences)\n",
    "\n",
    "        # Pass embeddings through CNN layer\n",
    "        conv_out = self.conv1d(embeddings.permute(0, 2, 1))  # Reshape for CNN\n",
    "        conv_out = torch.relu(conv_out)\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # Reshape back for LSTM\n",
    "\n",
    "        # Pass CNN output through LSTM layer\n",
    "        lstm_out, _ = self.lstm(conv_out)\n",
    "\n",
    "        # Final output layer\n",
    "        output = self.linear(lstm_out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        super(LSTM_CRF, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # BiLSTM layer\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(n_classes)\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "        # Create a CRF layer\n",
    "        self.crf = CRF(n_classes, batch_first=True)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # Input x is a sequence of indices\n",
    "        embedded = self.embedding(sentences)\n",
    "\n",
    "        # BiLSTM layer\n",
    "        lstm_out, _ = self.bilstm(embedded)\n",
    "\n",
    "        # Linear layer for classification\n",
    "        linear_out = self.linear(lstm_out)\n",
    "        if labels is not None:\n",
    "            # Calculate the negative log-likelihood loss using the CRF layer\n",
    "            loss = self.crf(output, labels)\n",
    "            return -loss  # Return negative log-likelihood as we usually minimize it during training\n",
    "        else:\n",
    "            # If labels are not provided, return the raw output\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=0.5):\n",
    "        super(RNN_CRF, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "        self.dropout = nn.Dropout(dropout)  # Apply dropout before the linear layer\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(n_classes)  # Place the CRF layer after the linear layer\n",
    "\n",
    "    def forward(self, word):\n",
    "        embeddings = self.embedding(word)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        dropout_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        output = self.linear(dropout_out)\n",
    "        return output  # Return raw output for CRF loss calculation\n",
    "\n",
    "    def predict(self, word):\n",
    "        output = self.forward(word)\n",
    "        predictions = self.crf.decode(output)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_CRF_Pre_Trained(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=0.5, pretrained_embedding=None, freeze_embedding=False):\n",
    "        super(RNN_CRF_Pre_Trained, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True).double()\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes).float()\n",
    "        self.dropout = nn.Dropout(dropout)  # Apply dropout before the linear layer\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(n_classes)  # Place the CRF layer after the linear layer\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        dropout_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        output = self.linear(dropout_out.float())\n",
    "        return output  # Return raw output for CRF loss calculation\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        output = self.forward(sentences)\n",
    "        predictions = self.crf.decode(output)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_CRF_MultiLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, word_vocab_size, n_classes, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=0.5):\n",
    "        super(RNN_CRF_MultiLayer, self).__init__()\n",
    "\n",
    "        # Word embedding layer\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, embedding_dim)\n",
    "\n",
    "        # Word LSTM layer\n",
    "        self.word_lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Character embedding layer (already exists)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Character LSTM layer (already exists)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Linear layer after concatenation\n",
    "        self.linear = nn.Linear(hidden_size * 4, n_classes)  # Combined output from both LSTMs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(n_classes)\n",
    "\n",
    "    def forward(self, sentences, words):\n",
    "        # Word layer processing\n",
    "        word_embeddings = self.word_embedding(words).float()\n",
    "        word_lstm_out, _ = self.word_lstm(word_embeddings)\n",
    "\n",
    "        # Character layer processing\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "\n",
    "        # Concatenate outputs\n",
    "        combined_output = torch.cat([lstm_out, word_lstm_out], dim=2)\n",
    "\n",
    "        # Linear transformation and CRF\n",
    "        dropout_out = self.dropout(combined_output)\n",
    "        output = self.linear(dropout_out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(\n",
    "    train_inputs, val_inputs, train_labels, val_labels, batch_size=BATCH_SIZE\n",
    "):\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, sampler=train_sampler, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "\n",
    "\n",
    "def train(\n",
    "    path, model, optimizer, train_dataloader, val_dataloader=None, epochs=NUM_EPOCHS\n",
    "):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "\n",
    "    # Tracking best validation accuracy\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(\n",
    "        f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\"\n",
    "    )\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "\n",
    "        # Tracking time and loss\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass\n",
    "            output = model(b_input_ids)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(output.view(-1, output.shape[-1]), b_labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if val_dataloader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Track the best accuracy\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), path)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(\n",
    "                f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\"\n",
    "            )\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"\n",
    "    After the completion of each training epoch, measure the model's\n",
    "    performance on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_loss = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Filter out the padding value\n",
    "        labels_without_pad = (b_labels != PAD)\n",
    "\n",
    "        # Get the output\n",
    "        with torch.no_grad():\n",
    "            output = model(b_input_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(output.view(-1, output.shape[-1]), b_labels.view(-1))\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = output.argmax(dim=2)\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        correct_predictions = ((preds == b_labels) & labels_without_pad).sum().item()\n",
    "        actual_predictions = labels_without_pad.sum().item()\n",
    "        accuracy = correct_predictions / actual_predictions\n",
    "\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def init_model(learning_rate=LEARNING_RATE):\n",
    "    model = CNN()\n",
    "    path = CNN_PATH\n",
    "\n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return path, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = gensim.models.Word2Vec.load('models/full_grams_cbow_300_twitter.mdl')\n",
    "embedding_dim = t_model.vector_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "aravec_embeddings_train = []\n",
    "corpus=  readFile(TRAIN_PATH)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for sentence in corpus[:100]:\n",
    "\t# Clean each sentence in the corpus\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\tchar_list, diacritics_list = separate_words_and_diacritics(sentence.strip())\n",
    "\twords = [''.join(sublist) for sublist in char_list]\n",
    "\twindows=get_all_windows(' '.join(words), WINDOW_SIZE_BEFORE, WINDOW_SIZE_AFTER)\n",
    "\t# print(windows)\n",
    "\tfor window in windows:\n",
    "\t\tembeddings = [t_model.wv[clean_str(word)] if clean_str(word) in t_model.wv else np.zeros(embedding_dim) for word in window]\n",
    "\t\t# print(np.mean(embeddings, axis=0))\n",
    "\t\taravec_embeddings_train.append(np.mean(embeddings, axis=0))\n",
    "\t# if(char_list)\n",
    "\t# print(char_list)\n",
    "\tx_train.append(char_list)\n",
    "\ty_train.append(diacritics_list)\n",
    "\n",
    "X_train_padded = [torch.tensor([char_to_index[char] for char in word]) for sentence in x_train for word in sentence]\n",
    "X_train_padded = pad_sequence(X_train_padded, batch_first=True)\n",
    "\n",
    "y_train_padded = [torch.tensor([diacritic_to_index[char] for char in word]) for sentence in y_train for word in sentence]\n",
    "y_train_padded = pad_sequence(y_train_padded, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "aravec_embeddings_train = []\n",
    "corpus=  readFile(TRAIN_PATH)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for sentence in corpus[:5]:\n",
    "\t# Clean each sentence in the corpus\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "    char_list, diacritics_list = separate_words_and_diacritics(sentence.strip())\n",
    "    # print(char_list)\n",
    "    joined_lists = [''.join(sublist) for sublist in char_list if sublist != []]\n",
    "    # print(joined_lists)\n",
    "    for ele in joined_lists:\n",
    "        ele_cleaned=clean_str(ele)\n",
    "        if ele_cleaned in t_model.wv:\n",
    "            aravec_embeddings_train.append(t_model.wv[ele_cleaned])\n",
    "            # print(t_model.wv[ele_cleaned])\n",
    "        else:\n",
    "            aravec_embeddings_train.append(np.zeros(embedding_dim))\n",
    "    x_train.append(char_list)\n",
    "    y_train.append(diacritics_list)\n",
    "\n",
    "X_train_padded = [torch.tensor([char_to_index[char] for char in word]) for sentence in x_train for word in sentence]\n",
    "X_train_padded = pad_sequence(X_train_padded, batch_first=True)\n",
    "\n",
    "y_train_padded = [torch.tensor([diacritic_to_index[char] for char in word]) for sentence in y_train for word in sentence]\n",
    "y_train_padded = pad_sequence(y_train_padded, batch_first=True,padding_value=PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "aravec_embeddings_val_test = []\n",
    "\n",
    "valid_corpus = readFile(VAL_PATH)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for sentence in valid_corpus[:2]:\n",
    "\t# Clean each sentence in the corpus\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\tchar_list, diacritics_list = separate_words_and_diacritics(sentence.strip())\n",
    "\twords = [''.join(sublist) for sublist in char_list]\n",
    "\tfor ele in words:\n",
    "\t\tif ele in t_model.wv:\n",
    "\t\t\taravec_embeddings_val_test.append(t_model.wv[ele])\n",
    "\t\telse:\n",
    "\t\t\taravec_embeddings_val_test.append(np.zeros(embedding_dim))\n",
    "\tX_val.append(char_list)\n",
    "\ty_val.append(diacritics_list)\n",
    "\n",
    "X_val_padded = [torch.tensor([char_to_index[char] for char in word]) for sentence in X_val for word in sentence ]\n",
    "X_val_padded = pad_sequence(X_val_padded, batch_first=True)\n",
    "\n",
    "y_val_padded = [torch.tensor([diacritic_to_index[char] for char in word]) for sentence in y_val for word in sentence ]\n",
    "y_val_padded = pad_sequence(y_val_padded, batch_first=True, padding_value=PAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(y_val_padded))\n",
    "# print(type(aravec_embeddings_val_test))\n",
    "aravec_embeddings_val_test=np.array(aravec_embeddings_val_test)\n",
    "# print(type(aravec_embeddings_val_test))\n",
    "\n",
    "# Create an index array\n",
    "# indices = list(range(len(X_val_padded)))\n",
    "\n",
    "# Split the indices into validation and test sets\n",
    "indices_val, indices_test = train_test_split(indices, test_size=0.5, random_state=42)\n",
    "# Use the indices to get the corresponding data for validation and test sets\n",
    "x_val = X_val_padded[indices_val]\n",
    "y_val = y_val_padded[indices_val]\n",
    "aravec_embeddings_val=aravec_embeddings_val_test[indices_val].tolist()\n",
    "\n",
    "x_test = X_val_padded[indices_test]\n",
    "y_test = y_val_padded[indices_test]\n",
    "aravec_embeddings_test=aravec_embeddings_val_test[indices_test].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RNN():\n",
    "    model=RNN(VOCAB_SIZE, LABELS_SIZE)\n",
    "    print(model)\n",
    "    train(model, LSTM_PATH, X_train_padded, y_train_padded)\n",
    "    \n",
    "def run_CNN():\n",
    "    model=CNN(VOCAB_SIZE, LABELS_SIZE)\n",
    "    print(model)\n",
    "    train(model, CNN_PATH, X_train_padded, y_train_padded)\n",
    "    \n",
    "def run_CNN_eslam():\n",
    "    model=RNN_CNN(VOCAB_SIZE, LABELS_SIZE)\n",
    "    print(model)\n",
    "    train(model,CNN_PATH,CNN_val_PATH, X_train_padded, y_train_padded,X_val,y_val)\n",
    "    \n",
    "def run_CRF():\n",
    "    model=LSTM_CRF(VOCAB_SIZE, LABELS_SIZE)\n",
    "    print(model)\n",
    "    train(model, CNN_PATH, X_train_padded, y_train_padded)\n",
    "def run_CRF_eslam():\n",
    "    model=RNN_CRF(VOCAB_SIZE, LABELS_SIZE)\n",
    "    print(model)\n",
    "    train(model,CRF_PATH,CRF_Val_PATH, X_train_padded, y_train_padded,x_val,y_val)\n",
    "def run_CRF_Pre_Trained():\n",
    "    model=RNN_CRF_Pre_Trained(VOCAB_SIZE, LABELS_SIZE,pretrained_embedding=torch.tensor(aravec_embeddings_train), freeze_embedding=True)\n",
    "    print(model)\n",
    "    train(model,CRF_PATH,CRF_Val_PATH, X_train_padded, y_train_padded,x_val,y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(learning_rate=LEARNING_RATE):\n",
    "    model = RNN_CRF_Pre_Trained(VOCAB_SIZE, LABELS_SIZE,pretrained_embedding=torch.tensor(aravec_embeddings_train), freeze_embedding=True)\n",
    "    path = CRF_PATH\n",
    "\n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return path, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   2.721539   |  2.312942  |   0.33    |   0.92   \n",
      "   2    |   2.292720   |  2.002950  |   0.35    |   0.64   \n",
      "   3    |   1.968677   |  1.831115  |   0.35    |   0.58   \n",
      "   4    |   1.774864   |  1.758043  |   0.39    |   0.74   \n",
      "   5    |   1.670308   |  1.735107  |   0.43    |   0.71   \n",
      "   6    |   1.640491   |  1.704063  |   0.46    |   0.74   \n",
      "   7    |   1.575503   |  1.671824  |   0.49    |   0.61   \n",
      "   8    |   1.527026   |  1.645846  |   0.48    |   0.59   \n",
      "   9    |   1.471922   |  1.620593  |   0.46    |   0.59   \n",
      "  10    |   1.404320   |  1.600271  |   0.45    |   0.61   \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 0.49%.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path, model, optimizer = init_model()\n",
    "train_dataloader, val_dataloader = data_loader(X_train_padded, X_val_padded, y_train_padded, y_val_padded)\n",
    "train(path, model, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

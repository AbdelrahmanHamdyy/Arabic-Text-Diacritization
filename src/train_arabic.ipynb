{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "379\n",
      "قَوْلُهُ لِعَدَمِ مَا تَتَعَلَّقُ إلَخْ أَيْ الْوَصِيَّةُ قَوْلُهُ مَا مَرَّ أَيْ قُبَيْلَ قَوْلِ الْمَتْنِ لَغَتْ وَلَوْ اقْتَصَرَ عَلَى أَوْصَيْت لَهُ بِشَاةٍ أَوْ أَعْطُوهُ شَاةً وَلَا غَنَمَ لَهُ عِنْدَ الْمَوْتِ هَلْ تَبْطُلُ الْوَصِيَّةُ أَوْ يُشْتَرَى لَهُ شَاةٌ وَيُؤْخَذُ مِنْ قَوْلِهِ الْآتِي كَمَا لَوْ لَمْ يَقُلْ مِنْ مَالِي وَلَا مِنْ غَنَمِي أَنَّهَا لَا تَبْطُلُ \n",
      "----------------------------\n",
      "234\n",
      " وَعِبَارَةُ الْكَنْزِ وَلَوْ لَمْ يَقُلْ مِنْ مَالِي وَلَا مِنْ غَنَمِي لَمْ يَتَعَيَّنْ غَنَمُهُ إنْ كَانَتْ انْتَهَتْ ا ه سم قَوْلُهُ فَيُعْطَى وَاحِدَةً مِنْهَا إلَخْ كَمَا لَوْ كَانَتْ مَوْجُودَةً عِنْدَ الْوَصِيَّةِ وَالْمَوْتِ \n",
      "----------------------------\n",
      "96\n",
      " وَلَا يَجُوزُ أَنْ يُعْطَى وَاحِدَةً مِنْ غَيْرِ غَنَمِهِ فِي الصُّورَتَيْنِ وَإِنْ تَرَاضَيَا \n",
      "----------------------------\n",
      "106\n",
      " لِأَنَّهُ صُلْحٌ عَلَى مَجْهُولٍ مُغْنِي وَنِهَايَةٌ قَالَ ع ش قَوْلُهُ وَاحِدَةً مِنْهَا أَيْ كَامِلَةً \n",
      "----------------------------\n",
      "54\n",
      " وَلَا يَجُوزُ أَنْ يُعْطَى نِصْفَيْنِ مِنْ شَاتَيْنِ \n",
      "----------------------------\n",
      "160\n",
      " لِأَنَّهُ لَا يُسَمَّى شَاةً وَقَوْلُهُ وَلَا يَجُوزُ أَنْ يُعْطَى وَاحِدَةً مِنْ غَيْرِ غَنَمِهِ وَيَنْبَغِي أَنْ يُقَالَ مِثْلُ ذَلِكَ فِي الْأَرِقَّاءِ ا ه \n",
      "----------------------------\n",
      "0\n",
      "\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from TorchCRF import CRF\n",
    "\n",
    "%run updatePreprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "VOCAB_SIZE = len(basic_arabic_letters) + 1\n",
    "LABELS_SIZE = len(DIACRITICS)\n",
    "\n",
    "TRAIN_PATH = \"../dataset/train.txt\"\n",
    "VAL_PATH = \"../dataset/val.txt\"\n",
    "LSTM_PATH=\"./models/lstm.pth\"\n",
    "RNN_PATH=\"./models/rnn.pth\"\n",
    "CNN_PATH = \"./models/cnn.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        \"\"\"\n",
    "        The constructor of our RNN model\n",
    "        Inputs:\n",
    "        - vocab_size: the number of unique characters\n",
    "        - embedding_dim: the embedding dimension\n",
    "        - n_classes: the number of final classes (diacritics)\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # (1) Create an embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # (3) Create a linear layer with number of neorons = n_classes\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        \"\"\"\n",
    "        This function does the forward pass of our model\n",
    "        Inputs:\n",
    "        - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "        Returns:\n",
    "        - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        final_output = None\n",
    "        \n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        # final_output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Max pooling layers\n",
    "        # self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"1\", x.shape)\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(\"2\", x.shape)\n",
    "        # Convolutional layers with ReLU activation and max pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # x = self.pool(x)\n",
    "        print(\"3\", x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # x = self.pool(x)\n",
    "        print(\"4\", x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # x = self.pool(x)\n",
    "        print(\"5\", x.shape)\n",
    "\n",
    "        # Fully connected layers with ReLU activation\n",
    "        x = x.view(-1, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(\"6\", x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print(\"7\", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        super(LSTM_CRF, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # BiLSTM layer\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(n_classes)\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "        # Create a CRF layer\n",
    "        self.crf = CRF(n_classes, batch_first=True)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # Input x is a sequence of indices\n",
    "        embedded = self.embedding(sentences)\n",
    "\n",
    "        # BiLSTM layer\n",
    "        lstm_out, _ = self.bilstm(embedded)\n",
    "\n",
    "        # Linear layer for classification\n",
    "        linear_out = self.linear(lstm_out)\n",
    "        if labels is not None:\n",
    "            # Calculate the negative log-likelihood loss using the CRF layer\n",
    "            loss = self.crf(output, labels)\n",
    "            return -loss  # Return negative log-likelihood as we usually minimize it during training\n",
    "        else:\n",
    "            # If labels are not provided, return the raw output\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\n",
    "        super(RNN_CRF, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_size * 2, n_classes)\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(n_classes)  # Place the CRF layer after the linear layer\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeddings = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output  # Return raw output for CRF loss calculation\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        output = self.forward(sentences)\n",
    "        predictions = self.crf.decode(output)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train(model, path, train_dataset, train_labels, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    This function implements the training logic\n",
    "    Inputs:\n",
    "    - model: the model to be trained\n",
    "    - train_dataset: the training set\n",
    "    - batch_size: integer represents the number of examples per step\n",
    "    - epochs: integer represents the total number of epochs (full training pass)\n",
    "    - learning_rate: the learning rate to be used by the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    tensor_train_dataset = TensorDataset(train_dataset, train_labels)\n",
    "    train_dataloader = DataLoader(tensor_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # (3) create the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # (4) create a learning rate scheduler (optional but recommended)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Adjust parameters as needed\n",
    "\n",
    "    # GPU configuration\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            # Zero your gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move the train input to the device\n",
    "            train_label = train_label.to(device)\n",
    "\n",
    "            # Move the train label to the device\n",
    "            train_input = train_input.to(device)\n",
    "\n",
    "            # Do the forward pass\n",
    "            output = model(train_input).float()\n",
    "\n",
    "            # Loss calculation\n",
    "            batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n",
    "\n",
    "            # Append the batch loss to the total_loss_train\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            # Calculate the batch accuracy (just add the number of correct predictions)\n",
    "            # Compare predicted diacritic with true diacritic and count correct predictions\n",
    "            correct_predictions = (output.argmax(dim=2) == train_label)\n",
    "\n",
    "            # Calculate accuracy for the current batch\n",
    "            acc = correct_predictions.sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            # Do the backward pass\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update the weights with your optimizer\n",
    "            optimizer.step()     \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "        # Calculate the epoch loss\n",
    "        epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        epoch_acc = total_acc_train / (len(train_dataset) * len(train_dataset[0]))\n",
    "\n",
    "        print(f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "            | Train Accuracy: {epoch_acc}\\n')\n",
    "        \n",
    "        if epoch_acc > best_accuracy:\n",
    "            best_accuracy = epoch_acc\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(f'Saved the best model with accuracy: {best_accuracy} to {path}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=  readFile(TRAIN_PATH)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for sentence in corpus[:1000]:\n",
    "\t# Clean each sentence in the corpus\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\t\tsplitted_sentences = get_splitted_sentences(sentence)\n",
    "\t\tfor splitted_sentence in splitted_sentences:\n",
    "\t\t\t\tif(len(splitted_sentence.strip())!=0):\n",
    "\t\t\t\t\t\tchar_list, diacritics_list = separate_words_and_diacritics(splitted_sentence.strip())\n",
    "\t\t\t\t\t\tx_train.append(char_list)\n",
    "\t\t\t\t\t\ty_train.append(diacritics_list)\n",
    "\n",
    "\n",
    "X_train_padded = [torch.tensor([char_to_index[char] for char in word]) for sentence in x_train for word in sentence]\n",
    "X_train_padded = pad_sequence(X_train_padded, batch_first=True)\n",
    "\n",
    "y_train_padded = [torch.tensor([diacritic_to_index[char] for char in word]) for sentence in y_train for word in sentence]\n",
    "y_train_padded = pad_sequence(y_train_padded, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RNN():\n",
    "    model=RNN(VOCAB_SIZE, LABELS_SIZE)\n",
    "    print(model)\n",
    "    train(model, LSTM_PATH, X_train_padded, y_train_padded)\n",
    "    \n",
    "def run_CNN():\n",
    "    model=CNN(VOCAB_SIZE, LABELS_SIZE)\n",
    "    print(model)\n",
    "    train(model, CNN_PATH, X_train_padded, y_train_padded)\n",
    "\n",
    "def run_CRF():\n",
    "    model=LSTM_CRF(VOCAB_SIZE, LABELS_SIZE)\n",
    "    print(model)\n",
    "    train(model, CNN_PATH, X_train_padded, y_train_padded)\n",
    "def run_CRF_eslam():\n",
    "    model=RNN_CRF(VOCAB_SIZE, LABELS_SIZE)\n",
    "    print(model)\n",
    "    train(model, CNN_PATH, X_train_padded, y_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(37, 250)\n",
      "  (lstm): LSTM(250, 512, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=1024, out_features=15, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:14<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.0013371970089965843             | Train Accuracy: 0.8857142857142857\n",
      "\n",
      "Saved the best model with accuracy: 0.8857142857142857 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:07<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.0006590705133362724             | Train Accuracy: 0.9416531281458372\n",
      "\n",
      "Saved the best model with accuracy: 0.9416531281458372 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:10<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.0005356372376437669             | Train Accuracy: 0.9522839401580767\n",
      "\n",
      "Saved the best model with accuracy: 0.9522839401580767 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:26<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 0.00046775068317036296             | Train Accuracy: 0.9579332409723814\n",
      "\n",
      "Saved the best model with accuracy: 0.9579332409723814 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:28<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss: 0.0004220452671406             | Train Accuracy: 0.9618163877565259\n",
      "\n",
      "Saved the best model with accuracy: 0.9618163877565259 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:42<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss: 0.0003545136618883434             | Train Accuracy: 0.9689627333526029\n",
      "\n",
      "Saved the best model with accuracy: 0.9689627333526029 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:21<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss: 0.0003398162963332582             | Train Accuracy: 0.9703481476313403\n",
      "\n",
      "Saved the best model with accuracy: 0.9703481476313403 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:16<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss: 0.00033173378446282246             | Train Accuracy: 0.9709880296219513\n",
      "\n",
      "Saved the best model with accuracy: 0.9709880296219513 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:41<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss: 0.0003269678354426678             | Train Accuracy: 0.9713089673181768\n",
      "\n",
      "Saved the best model with accuracy: 0.9713089673181768 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [04:04<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss: 0.0003218530860337729             | Train Accuracy: 0.9718053243763144\n",
      "\n",
      "Saved the best model with accuracy: 0.9718053243763144 to ./models/lstm.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_RNN()\n",
    "# run_CNN()\n",
    "# run_CRF()\n",
    "#run_CRF_eslam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique characters and diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ا': 1, 'ب': 2, 'ت': 3, 'ث': 4, 'ج': 5, 'ح': 6, 'خ': 7, 'د': 8, 'ذ': 9, 'ر': 10, 'ز': 11, 'س': 12, 'ش': 13, 'ص': 14, 'ض': 15, 'ط': 16, 'ظ': 17, 'ع': 18, 'غ': 19, 'ف': 20, 'ق': 21, 'ك': 22, 'ل': 23, 'م': 24, 'ن': 25, 'ه': 26, 'و': 27, 'ي': 28, 'ء': 29, 'أ': 30, 'إ': 31, 'ؤ': 32, 'ئ': 33, 'آ': 34, 'ٱ': 35, 'ٰ': 36, 'ى': 37, 'ة': 38}\n",
      "{' ٌّ': 0, 'ِ': 1, 'ُ': 2, ' َّ': 3, ' ِّ': 4, ' ُّ': 5, 'َ': 6, ' ًّ': 7, ' ٍّ': 8, 'ٌ': 9, 'ْ': 10, 'ٍ': 11, 'ً': 12, ' ': 13, 'ّ': 14}\n"
     ]
    }
   ],
   "source": [
    "unique_characters = ['A', 'b', 't', 'v', 'j', 'H', 'x', 'd', '*', 'r', 'z', 's', '$', 'S', 'D', 'T', 'Z', 'E', 'g', 'f', 'q', 'k', 'l', 'm', 'n', 'h', 'w', 'y', \"'\", '>', '<', '&', '}', '|', '{', '`', 'Y', 'p']\n",
    "\n",
    "arabic_letters = ['ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'أ', 'إ', 'ؤ', 'ئ', 'آ', 'ٱ', 'ٰ', 'ى', 'ة']\n",
    "\n",
    "unique_characters = arabic_letters\n",
    "unique_diacritics = DIACRITICS\n",
    "\n",
    "char_to_index = {char: i + 1 for i, char in enumerate(unique_characters)}\n",
    "diacritic_to_index = {diacritic: i for i, diacritic in enumerate(unique_diacritics)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path):\n",
    "\tsentences = []\n",
    "\twith open(path, 'r', encoding='utf-8') as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tsentences.append(line.strip())\n",
    "\n",
    "\treturn sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../dataset/train.txt\"\n",
    "VAL_PATH = \"../dataset/val.txt\"\n",
    "LSTM_PATH=\"./models/lstm.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_data():\n",
    "\tcorpus = readFile(TRAIN_PATH)\n",
    "\tloaded_model = FastText.load(\"./models/ft_model\")\n",
    "\tX_train = []\n",
    "\ty_train = []\n",
    "\tembeddings_train = []\n",
    "\tmax_sequence_length = 0\n",
    "\n",
    "\tfor sentence in corpus:\n",
    "\t\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\t\tchar_list, diacritics_list = separate_words_and_diacritics(sentence)\n",
    "\n",
    "\t\tX_train.append(char_list)\n",
    "\t\ty_train.append(diacritics_list)\n",
    "\n",
    "\t\t# Get the max sequence length and concatenate the embeddings of the words\n",
    "\t\tfor word in char_list:\n",
    "\t\t\tmax_sequence_length = max(max_sequence_length, len(word))\n",
    "\n",
    "\t\t\tembeddings_train.append(loaded_model.wv[word])\n",
    "\n",
    "\t# embeddings_train = np.concatenate(embeddings_train, axis=0)\n",
    "\tprint(np.array(embeddings_train).shape)\n",
    "\treturn X_train, y_train, embeddings_train, max_sequence_length\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_data( X_train, y_train,max_sequence_length):\n",
    "\t# Encoding and Padding the data\n",
    "\tX_train_padded = []\n",
    "\tfor sentence in X_train:\n",
    "\t\tX_train_sequences = [[char_to_index[char] for char in word] for word in sentence]\n",
    "\t\tX_train_padded.append(pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "\t# X_train_padded = np.concatenate(X_train_padded, axis=0)\n",
    "\tprint(np.array(X_train_padded).shape)\n",
    "\n",
    "\ty_train_padded = []\n",
    "\tfor sentence in y_train:\n",
    "\t\ty_train_sequences = [[diacritic_to_index[diacritic] for diacritic in diacritic_sequence] for diacritic_sequence in sentence]\n",
    "\t\ty_train_padded.append(pad_sequences(y_train_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "\t# y_train_padded = np.concatenate(y_train_padded, axis=0)\n",
    "\tprint(np.array(y_train_padded).shape)\n",
    "\treturn X_train_padded, y_train_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_val():\n",
    "    val_corpus = readFile(VAL_PATH)\n",
    "    loaded_model = FastText.load(\"./models/ft_model\")\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    embeddings_train = []\n",
    "    for sentence in val_corpus:\n",
    "        # Clean each sentence in the corpus\n",
    "        clean_sentence = run_buckwalter(sentence.strip())\n",
    "        # Get the char list for each word in the sentence and its corresponding diacritics\n",
    "        char_list, diacritics_list = extract_labels(clean_sentence)\n",
    "\n",
    "        X_val.append(char_list)\n",
    "        y_val.append(diacritics_list)\n",
    "        for word in char_list:\n",
    "\n",
    "            embeddings_train.append(loaded_model.wv[word])\n",
    "    return X_val, y_val,embeddings_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_val(X_val, y_val, max_sequence_length):\n",
    "    # Encoding and Padding the data\n",
    "    X_val_padded = []\n",
    "    for sentence in X_val:\n",
    "        X_val_sequences = [[char_to_index[char] for char in word] for word in sentence]\n",
    "        X_val_padded.append(pad_sequences(X_val_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "    # X_val_padded = np.concatenate(X_val_padded, axis=0)\n",
    "    print(np.array(X_val_padded).shape)\n",
    "\n",
    "    y_val_padded = []\n",
    "    for sentence in y_val:\n",
    "        y_val_sequences = [[diacritic_to_index[diacritic] for diacritic in diacritic_sequence] for diacritic_sequence in sentence]\n",
    "        y_val_padded.append(pad_sequences(y_val_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "    # y_val_padded = np.concatenate(y_val_padded, axis=0)\n",
    "    print(np.array(y_val_padded).shape)\n",
    "    return X_val_padded, y_val_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

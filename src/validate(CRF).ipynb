{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ق', 'و', 'ل', 'ه'], ['ل', 'ع', 'د', 'م'], ['م', 'ا'], ['ت', 'ت', 'ع', 'ل', 'ق'], ['إ', 'ل', 'خ'], ['أ', 'ي'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['ق', 'و', 'ل', 'ه'], ['م', 'ا'], ['م', 'ر'], ['أ', 'ي'], ['ق', 'ب', 'ي', 'ل'], ['ق', 'و', 'ل'], ['ا', 'ل', 'م', 'ت', 'ن'], ['ل', 'غ', 'ت'], ['و', 'ل', 'و'], ['ا', 'ق', 'ت', 'ص', 'ر'], ['ع', 'ل', 'ى'], ['أ', 'و', 'ص', 'ي', 'ت'], ['ل', 'ه'], ['ب', 'ش', 'ا', 'ة'], ['أ', 'و'], ['أ', 'ع', 'ط', 'و', 'ه'], ['ش', 'ا', 'ة'], ['و', 'ل', 'ا'], ['غ', 'ن', 'م'], ['ل', 'ه'], ['ع', 'ن', 'د'], ['ا', 'ل', 'م', 'و', 'ت'], ['ه', 'ل'], ['ت', 'ب', 'ط', 'ل'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['أ', 'و'], ['ي', 'ش', 'ت', 'ر', 'ى'], ['ل', 'ه'], ['ش', 'ا', 'ة'], ['و', 'ي', 'ؤ', 'خ', 'ذ'], ['م', 'ن'], ['ق', 'و', 'ل', 'ه'], ['ا', 'ل', 'آ', 'ت', 'ي'], ['ك', 'م', 'ا'], ['ل', 'و'], ['ل', 'م'], ['ي', 'ق', 'ل'], ['م', 'ن'], ['م', 'ا', 'ل', 'ي'], ['و', 'ل', 'ا'], ['م', 'ن'], ['غ', 'ن', 'م', 'ي'], ['أ', 'ن', 'ه', 'ا'], ['ل', 'ا'], ['ت', 'ب', 'ط', 'ل'], ['و', 'ع', 'ب', 'ا', 'ر', 'ة'], ['ا', 'ل', 'ك', 'ن', 'ز'], ['و', 'ل', 'و'], ['ل', 'م'], ['ي', 'ق', 'ل'], ['م', 'ن'], ['م', 'ا', 'ل', 'ي'], ['و', 'ل', 'ا'], ['م', 'ن'], ['غ', 'ن', 'م', 'ي'], ['ل', 'م'], ['ي', 'ت', 'ع', 'ي', 'ن'], ['غ', 'ن', 'م', 'ه'], ['إ', 'ن'], ['ك', 'ا', 'ن', 'ت'], ['ا', 'ن', 'ت', 'ه', 'ت'], [], [], ['س', 'م'], ['ق', 'و', 'ل', 'ه'], ['ف', 'ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن', 'ه', 'ا'], ['إ', 'ل', 'خ'], ['ك', 'م', 'ا'], ['ل', 'و'], ['ك', 'ا', 'ن', 'ت'], ['م', 'و', 'ج', 'و', 'د', 'ة'], ['ع', 'ن', 'د'], ['ا', 'ل', 'و', 'ص', 'ي', 'ة'], ['و', 'ا', 'ل', 'م', 'و', 'ت'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن'], ['غ', 'ي', 'ر'], ['غ', 'ن', 'م', 'ه'], ['ف', 'ي'], ['ا', 'ل', 'ص', 'و', 'ر', 'ت', 'ي', 'ن'], ['و', 'إ', 'ن'], ['ت', 'ر', 'ا', 'ض', 'ي', 'ا'], ['ل', 'أ', 'ن', 'ه'], ['ص', 'ل', 'ح'], ['ع', 'ل', 'ى'], ['م', 'ج', 'ه', 'و', 'ل'], ['م', 'غ', 'ن', 'ي'], ['و', 'ن', 'ه', 'ا', 'ي', 'ة'], ['ق', 'ا', 'ل'], [], [], ['ق', 'و', 'ل', 'ه'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن', 'ه', 'ا'], ['أ', 'ي'], ['ك', 'ا', 'م', 'ل', 'ة'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['ن', 'ص', 'ف', 'ي', 'ن'], ['م', 'ن'], ['ش', 'ا', 'ت', 'ي', 'ن'], ['ل', 'أ', 'ن', 'ه'], ['ل', 'ا'], ['ي', 'س', 'م', 'ى'], ['ش', 'ا', 'ة'], ['و', 'ق', 'و', 'ل', 'ه'], ['و', 'ل', 'ا'], ['ي', 'ج', 'و', 'ز'], ['أ', 'ن'], ['ي', 'ع', 'ط', 'ى'], ['و', 'ا', 'ح', 'د', 'ة'], ['م', 'ن'], ['غ', 'ي', 'ر'], ['غ', 'ن', 'م', 'ه'], ['و', 'ي', 'ن', 'ب', 'غ', 'ي'], ['أ', 'ن'], ['ي', 'ق', 'ا', 'ل'], ['م', 'ث', 'ل'], ['ذ', 'ل', 'ك'], ['ف', 'ي'], ['ا', 'ل', 'أ', 'ر', 'ق', 'ا', 'ء'], [], []]\n",
      "[['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'َ', 'ِ'], ['َ', ' '], ['َ', 'َ', 'َ', 'َّ', 'ُ'], [' ', 'َ', 'ْ'], ['َ', 'ْ'], [' ', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', ' '], ['َ', 'َّ'], ['َ', 'ْ'], ['ُ', 'َ', 'ْ', 'َ'], ['َ', 'ْ', 'ِ'], [' ', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'َ', 'ْ'], [' ', 'ْ', 'َ', 'َ', 'َ'], ['َ', 'َ', ' '], ['َ', 'ْ', 'َ', 'ْ', ' '], ['َ', 'ُ'], ['ِ', 'َ', ' ', 'ٍ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', ' ', 'ُ'], ['َ', ' ', 'ً'], ['َ', 'َ', ' '], ['َ', 'َ', 'َ'], ['َ', 'ُ'], ['ِ', 'ْ', 'َ'], [' ', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', 'ُ'], [' ', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', ' '], ['َ', 'ُ'], ['َ', ' ', 'ٌ'], ['َ', 'ُ', 'ْ', 'َ', 'ُ'], ['ِ', 'ْ'], ['َ', 'ْ', 'ِ', 'ِ'], [' ', 'ْ', ' ', 'ِ', ' '], ['َ', 'َ', ' '], ['َ', 'ْ'], ['َ', 'ْ'], ['َ', 'ُ', 'ْ'], ['ِ', 'ْ'], ['َ', ' ', 'ِ', ' '], ['َ', 'َ', ' '], ['ِ', 'ْ'], ['َ', 'َ', 'ِ', ' '], ['َ', 'َّ', 'َ', ' '], ['َ', ' '], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ِ', 'َ', ' ', 'َ', 'ُ'], [' ', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'ْ'], ['َ', 'ُ', 'ْ'], ['ِ', 'ْ'], ['َ', ' ', 'ِ', ' '], ['َ', 'َ', ' '], ['ِ', 'ْ'], ['َ', 'َ', 'ِ', ' '], ['َ', 'ْ'], ['َ', 'َ', 'َ', 'َّ', 'ْ'], ['َ', 'َ', 'ُ', 'ُ'], [' ', 'ْ'], ['َ', ' ', 'َ', 'ْ'], [' ', 'ْ', 'َ', 'َ', 'ْ'], [], [], [' ', ' '], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ُ', 'ْ', 'َ', ' '], ['َ', ' ', 'ِ', 'َ', 'ً'], ['ِ', 'ْ', 'َ', ' '], [' ', 'َ', 'ْ'], ['َ', 'َ', ' '], ['َ', 'ْ'], ['َ', ' ', 'َ', 'ْ'], ['َ', 'ْ', 'ُ', ' ', 'َ', 'ً'], ['ِ', 'ْ', 'َ'], [' ', 'ْ', 'َ', 'ِ', 'َّ', 'ِ'], ['َ', ' ', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', ' '], ['َ', 'ُ', ' ', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', ' '], ['َ', ' ', 'ِ', 'َ', 'ً'], ['ِ', 'ْ'], ['َ', 'ْ', 'ِ'], ['َ', 'َ', 'ِ', 'ِ'], ['ِ', ' '], [' ', ' ', 'ُّ', ' ', 'َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'ْ'], ['َ', 'َ', ' ', 'َ', 'َ', ' '], ['ِ', 'َ', 'َّ', 'ُ'], ['ُ', 'ْ', 'ٌ'], ['َ', 'َ', ' '], ['َ', 'ْ', 'ُ', ' ', 'ٍ'], ['ُ', 'ْ', 'ِ', ' '], ['َ', 'ِ', 'َ', ' ', 'َ', 'ٌ'], ['َ', ' ', 'َ'], [], [], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', ' ', 'ِ', 'َ', 'ً'], ['ِ', 'ْ', 'َ', ' '], ['َ', 'ْ'], ['َ', ' ', 'ِ', 'َ', 'ً'], ['َ', 'َ', ' '], ['َ', 'ُ', ' ', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', ' '], ['ِ', 'ْ', 'َ', 'ْ', 'ِ'], ['ِ', 'ْ'], ['َ', ' ', 'َ', 'ْ', 'ِ'], ['ِ', 'َ', 'َّ', 'ُ'], ['َ', ' '], ['ُ', 'َ', 'َّ', ' '], ['َ', ' ', 'ً'], ['َ', 'َ', 'ْ', 'ُ', 'ُ'], ['َ', 'َ', ' '], ['َ', 'ُ', ' ', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', ' '], ['َ', ' ', 'ِ', 'َ', 'ً'], ['ِ', 'ْ'], ['َ', 'ْ', 'ِ'], ['َ', 'َ', 'ِ', 'ِ'], ['َ', 'َ', 'ْ', 'َ', 'ِ', ' '], ['َ', 'ْ'], ['ُ', 'َ', ' ', 'َ'], ['ِ', 'ْ', 'ُ'], ['َ', 'ِ', 'َ'], ['ِ', ' '], [' ', 'ْ', 'َ', 'ِ', 'َّ', ' ', 'ِ'], [], []]\n",
      "CBHGModel(\n",
      "  (embedding): Embedding(37, 200)\n",
      "  (cbhg): CBHG(\n",
      "    (relu): ReLU()\n",
      "    (conv1d_banks): ModuleList(\n",
      "      (0): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (3): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(4,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (4): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (5): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(6,), stride=(1,), padding=(3,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (6): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (7): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(8,), stride=(1,), padding=(4,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (8): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (9): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(10,), stride=(1,), padding=(5,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (10): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (11): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(12,), stride=(1,), padding=(6,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (12): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(13,), stride=(1,), padding=(6,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (13): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(14,), stride=(1,), padding=(7,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (14): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (15): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(200, 200, kernel_size=(16,), stride=(1,), padding=(8,), bias=False)\n",
      "        (bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (max_pool1d): MaxPool1d(kernel_size=2, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv1d_projections): ModuleList(\n",
      "      (0): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(3200, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): BatchNormConv1d(\n",
      "        (conv1d): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (pre_highway): Linear(in_features=256, out_features=200, bias=False)\n",
      "    (highways): ModuleList(\n",
      "      (0): Highway(\n",
      "        (H): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (T): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (1): Highway(\n",
      "        (H): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (T): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (2): Highway(\n",
      "        (H): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (T): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "      (3): Highway(\n",
      "        (H): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (T): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (sigmoid): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (gru): GRU(200, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (post_cbhg_layers): ModuleList(\n",
      "    (0): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "    (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (projections): Linear(in_features=512, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# %run train.ipynb\n",
    "# %run train_arabic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_dataset, val_labels, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    This function implements the validation logic\n",
    "    Inputs:\n",
    "    - model: the trained model\n",
    "    - val_dataset: the validation set\n",
    "    - batch_size: integer representing the number of examples per step\n",
    "    \"\"\"\n",
    "\n",
    "    # (1) create the dataloader for the validation set (make shuffle=False)\n",
    "    tensor_val_dataset = TensorDataset(val_dataset, val_labels)\n",
    "    val_dataloader = DataLoader(tensor_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # GPU configuration\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_input, val_label in tqdm(val_dataloader):\n",
    "\n",
    "            # Move the validation input to the device\n",
    "            val_label = val_label.to(device)\n",
    "\n",
    "            # Move the validation label to the device\n",
    "            val_input = val_input.to(device)\n",
    "\n",
    "            # Do the forward pass\n",
    "            output = model(val_input).float()\n",
    "\n",
    "            # Calculate the batch accuracy\n",
    "            correct_predictions = (output.argmax(dim=2) == val_label)\n",
    "            acc = correct_predictions.sum().item()\n",
    "            total_acc_val += acc\n",
    "\n",
    "    # Calculate metrics for the entire validation set\n",
    "    val_accuracy = total_acc_val / (len(val_dataset) * len(val_dataset[0]))\n",
    "\n",
    "    print(f'Validation Accuracy: {val_accuracy} | DER: {1 - val_accuracy}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_corpus = readFile(VAL_PATH)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for sentence in valid_corpus:\n",
    "\t# Clean each sentence in the corpus\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\tchar_list, diacritics_list = separate_words_and_diacritics(sentence.strip())\n",
    "\n",
    "\tX_val.append(char_list)\n",
    "\ty_val.append(diacritics_list)\n",
    "\n",
    "X_val_padded = [torch.tensor([char_to_index[char] for char in word]) for sentence in X_val for word in sentence ]\n",
    "X_val_padded = pad_sequence(X_val_padded, batch_first=True)\n",
    "\n",
    "y_val_padded = [torch.tensor([diacritic_to_index[char] for char in word]) for sentence in y_val for word in sentence ]\n",
    "# print(y_val_padded)\n",
    "y_val_padded = pad_sequence(y_val_padded, batch_first=True)\n",
    "# print(y_val_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the saved RNN model for inference\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/crf.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m() \n\u001b[0;32m      4\u001b[0m validate(loaded_cbhg_model, X_val_padded, y_val_padded)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "# Load the saved RNN model for inference\n",
    "model = torch.load(\"./models/crf.pth\")\n",
    "model.eval() \n",
    "validate(loaded_cbhg_model, X_val_padded, y_val_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "15\n",
      "Char [['m', 's', '>', 'l', 'p'], ['w', 'm', 'n'], ['H', 'n', 'v'], ['w', 'h', 'w'], ['q', 'A', 'd', 'r'], ['E', 'l', 'Y'], ['A', 'l', '<', 'T', 'E', 'A', 'm'], ['>', 'w'], ['A', 'l', 'k', 's', 'w', 'p'], ['>', 'w'], ['A', 'l', 'E', 't', 'q'], ['v', 'm'], ['A', 'f', 't', 'q', 'r'], ['f', 'E', 'j', 'z'], ['E', 'n'], ['k', 'l'], ['*', 'l', 'k'], ['l', 'm'], ['y', 'j', 'z', 'h'], ['A', 'l', 'S', 'w', 'm'], ['>', 'S', 'l', 'A']]\n",
      "Diac [['a', 'o', 'a', 'a', 'N'], ['a', 'a', 'o'], ['a', 'i', 'a'], ['a', 'u', 'a'], ['a', ' ', 'i', 'N'], ['a', 'a', ' '], [' ', ' ', 'i', 'o', 'a', ' ', 'i'], ['a', 'o'], [' ', 'o', 'i', 'o', 'a', 'i'], ['a', 'o'], [' ', 'o', 'i', 'o', 'i'], ['u', '~a'], [' ', 'o', 'a', 'a', 'a'], ['a', 'a', 'a', 'a'], ['a', 'o'], ['u', '~i'], ['a', 'i', 'a'], ['a', 'o'], ['u', 'o', 'i', 'i'], [' ', ' ', '~a', 'o', 'u'], ['a', 'o', 'F', ' ']]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique characters and diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "unique_characters = ['A', 'b', 't', 'v', 'j', 'H', 'x', 'd', '*', 'r', 'z', 's', '$', 'S', 'D', 'T', 'Z', 'E', 'g', 'f', 'q', 'k', 'l', 'm', 'n', 'h', 'w', 'y', \"'\", '>', '<', '&', '}', '|', '{', '`', 'Y', 'p']\n",
    "unique_diacritics = ['o', 'a', 'i', '~', 'u', 'N', 'F', 'K', ' ', '~a', '~i', '~u', '~N', '~F', '~K']\n",
    "\n",
    "num_chars = len(unique_characters)\n",
    "num_classes = len(unique_diacritics)\n",
    "\n",
    "char_to_index = {char: i for i, char in enumerate(unique_characters)}\n",
    "diacritic_to_index = {diacritic: i for i, diacritic in enumerate(unique_diacritics)}\n",
    "\n",
    "print(num_chars)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path):\n",
    "\tsentences = []\n",
    "\twith open(path, 'r', encoding='utf-8') as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tsentences.append(line.strip())\n",
    "\n",
    "\treturn sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../dataset/train.txt\"\n",
    "VAL_PATH = \"../dataset/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = readFile(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FastText word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = FastText.load(\"./models/ft_model\")\n",
    "def get_word_embeddings(word):\n",
    "    return loaded_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8351478, 100)\n",
      "(2102068, 13)\n",
      "(2102068, 13)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "embeddings_train = []\n",
    "max_sequence_length = 0\n",
    "\n",
    "for sentence in corpus:\n",
    "\t# Clean each sentence in the corpus\n",
    "\tclean_sentence = run_buckwalter(sentence.strip())\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\tchar_list, diacritics_list = extract_labels(clean_sentence)\n",
    "\n",
    "\tX_train.append(char_list)\n",
    "\ty_train.append(diacritics_list)\n",
    "\n",
    "\t# Get the max sequence length and concatenate the embeddings of the words\n",
    "\tfor word in char_list:\n",
    "\t\tmax_sequence_length = max(max_sequence_length, len(word))\n",
    "\n",
    "\t\tembeddings_train.append(get_word_embeddings(word))\n",
    "\n",
    "embeddings_train = np.concatenate(embeddings_train, axis=0)\n",
    "print(embeddings_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get X_train and Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and Padding the data\n",
    "X_train_padded = []\n",
    "for sentence in X_train:\n",
    "\tX_train_sequences = [[char_to_index[char] for char in word] for word in sentence]\n",
    "\tX_train_padded.append(pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "X_train_padded = np.concatenate(X_train_padded, axis=0)\n",
    "print(X_train_padded.shape)\n",
    "\n",
    "y_train_padded = []\n",
    "for sentence in y_train:\n",
    "\ty_train_sequences = [[diacritic_to_index[diacritic] for diacritic in diacritic_sequence] for diacritic_sequence in sentence]\n",
    "\ty_train_padded.append(pad_sequences(y_train_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "y_train_padded = np.concatenate(y_train_padded, axis=0)\n",
    "print(y_train_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106066, 13)\n",
      "(106066, 13)\n"
     ]
    }
   ],
   "source": [
    "val_corpus = readFile(VAL_PATH)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for sentence in val_corpus:\n",
    "\t# Clean each sentence in the corpus\n",
    "\tclean_sentence = run_buckwalter(sentence.strip())\n",
    "\t# Get the char list for each word in the sentence and its corresponding diacritics\n",
    "\tchar_list, diacritics_list = extract_labels(clean_sentence)\n",
    "\n",
    "\tX_val.append(char_list)\n",
    "\ty_val.append(diacritics_list)\n",
    "\n",
    "# Encoding and Padding the data\n",
    "X_val_padded = []\n",
    "for sentence in X_val:\n",
    "\tX_val_sequences = [[char_to_index[char] for char in word] for word in sentence]\n",
    "\tX_val_padded.append(pad_sequences(X_val_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "X_val_padded = np.concatenate(X_val_padded, axis=0)\n",
    "print(X_val_padded.shape)\n",
    "\n",
    "y_val_padded = []\n",
    "for sentence in y_val:\n",
    "\ty_val_sequences = [[diacritic_to_index[diacritic] for diacritic in diacritic_sequence] for diacritic_sequence in sentence]\n",
    "\ty_val_padded.append(pad_sequences(y_val_sequences, maxlen=max_sequence_length, padding='post'))\n",
    "\n",
    "y_val_padded = np.concatenate(y_val_padded, axis=0)\n",
    "print(y_val_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "65690/65690 [==============================] - 649s 10ms/step - loss: 0.3270 - accuracy: 0.8807 - val_loss: 0.3076 - val_accuracy: 0.8862\n",
      "Epoch 2/5\n",
      "65690/65690 [==============================] - 616s 9ms/step - loss: 0.3034 - accuracy: 0.8873 - val_loss: 0.3024 - val_accuracy: 0.8872\n",
      "Epoch 3/5\n",
      "65690/65690 [==============================] - 614s 9ms/step - loss: 0.2991 - accuracy: 0.8884 - val_loss: 0.2989 - val_accuracy: 0.8888\n",
      "Epoch 4/5\n",
      "65690/65690 [==============================] - 603s 9ms/step - loss: 0.2971 - accuracy: 0.8890 - val_loss: 0.2977 - val_accuracy: 0.8890\n",
      "Epoch 5/5\n",
      "65690/65690 [==============================] - 606s 9ms/step - loss: 0.2959 - accuracy: 0.8893 - val_loss: 0.2977 - val_accuracy: 0.8886\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2500,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[245], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_padded, y_train_padded, epochs\u001b[38;5;241m=\u001b[39mnum_epochs, validation_data\u001b[38;5;241m=\u001b[39m(X_val_padded, y_val_padded))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate your model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\data_adapter.py:714\u001b[0m, in \u001b[0;36mListsOfScalarsDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    705\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    712\u001b[0m ):\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m         y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2500,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Define your model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "\t\t\t\t\tinput_dim=embeddings_train.shape[0],\n",
    "\t\t\t\t\toutput_dim=embedding_dim,\n",
    "\t\t\t\t\tinput_length=max_sequence_length,\n",
    "\t\t\t\t\tweights=[embeddings_train],\n",
    "\t\t\t\t\ttrainable=False\n",
    "\t\t\t\t\t))\n",
    "model.add(LSTM(units=100, return_sequences=True))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train your model\n",
    "model.fit(X_train_padded, y_train_padded, epochs=num_epochs, validation_data=(X_val_padded, y_val_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3315/3315 [==============================] - 13s 4ms/step - loss: 0.2977 - accuracy: 0.8886\n",
      "0.8886049389839172\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "وَعَلَيْهِ يَنْبَغِي حِمْلَ قوْلُ أَشَهُبُ \n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_val_padded, y_val_padded)[1]\n",
    "print(accuracy)\n",
    "\n",
    "sentence = \"وعليه ينبغي حمل قول أشهب\"\n",
    "clean_sentence = run_buckwalter(sentence)\n",
    "char_list, _ = extract_labels(clean_sentence)\n",
    "\n",
    "X_test_sequences = [[char_to_index[char] for char in word] for word in char_list]\n",
    "X_test = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "final_output = []\n",
    "index = 0\n",
    "index_to_diacritic = {index: diacritic for diacritic, index in diacritic_to_index.items()}\n",
    "\n",
    "\n",
    "new = sentence.split()\n",
    "index = 0\n",
    "\n",
    "for word in predictions:\n",
    "\tfor i in range(len(new[index])):\n",
    "\t\tfinal_output.append(new[index][i])\n",
    "\t\tmax_index = np.array(word[i]).argmax()\n",
    "\n",
    "\t\tif index_to_diacritic[max_index] != \" \":\n",
    "\t\t\tfinal_output.append(buckwalter.untransliterate(index_to_diacritic[max_index]))\n",
    "\t\n",
    "\tindex += 1\n",
    "\tfinal_output.append(\" \")\n",
    "\n",
    "final_output = \"\".join(final_output)\n",
    "\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
